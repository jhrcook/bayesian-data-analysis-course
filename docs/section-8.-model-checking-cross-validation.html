<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>8 Section 8. Model checking &amp; Cross-validation | bookdown</title>
  <meta name="description" content="" />
  <meta name="generator" content="8 Section 8. Model checking &amp; Cross-validation | bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="8 Section 8. Model checking &amp; Cross-validation | bookdown" />
  <meta property="og:type" content="book" />
  
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="8 Section 8. Model checking &amp; Cross-validation | bookdown" />
  
  
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="section-7.-hierarchical-models-and-exchangeability.html"/>
<link rel="next" href="section-9.-model-comparison-and-selection.html"/>
<script src="libs/header-attrs-2.10/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./">Notes for BDA3</a></strong></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Bayesian Data Analysis course</a>
<ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#resources"><i class="fa fa-check"></i><b>0.1</b> Resources</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#how-to-study"><i class="fa fa-check"></i><b>0.2</b> How to study</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#course-sections"><i class="fa fa-check"></i><b>0.3</b> Course sections</a></li>
<li class="chapter" data-level="0.4" data-path="index.html"><a href="index.html#additional-notes"><i class="fa fa-check"></i><b>0.4</b> Additional notes</a></li>
<li class="chapter" data-level="0.5" data-path="index.html"><a href="index.html#assignments-and-exercises"><i class="fa fa-check"></i><b>0.5</b> Assignments and exercises</a></li>
<li class="chapter" data-level="0.6" data-path="index.html"><a href="index.html#stan-models"><i class="fa fa-check"></i><b>0.6</b> Stan models</a></li>
</ul></li>
<li class="part"><span><b>I Notes</b></span></li>
<li class="chapter" data-level="" data-path="notes-introduction.html"><a href="notes-introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="1" data-path="section-1.-course-introduction-and-prerequisites.html"><a href="section-1.-course-introduction-and-prerequisites.html"><i class="fa fa-check"></i><b>1</b> Section 1. Course introduction and prerequisites</a>
<ul>
<li class="chapter" data-level="1.1" data-path="section-1.-course-introduction-and-prerequisites.html"><a href="section-1.-course-introduction-and-prerequisites.html#resources-1"><i class="fa fa-check"></i><b>1.1</b> Resources</a></li>
<li class="chapter" data-level="1.2" data-path="section-1.-course-introduction-and-prerequisites.html"><a href="section-1.-course-introduction-and-prerequisites.html#notes"><i class="fa fa-check"></i><b>1.2</b> Notes</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="section-1.-course-introduction-and-prerequisites.html"><a href="section-1.-course-introduction-and-prerequisites.html#reading-instructions"><i class="fa fa-check"></i><b>1.2.1</b> Reading instructions</a></li>
<li class="chapter" data-level="1.2.2" data-path="section-1.-course-introduction-and-prerequisites.html"><a href="section-1.-course-introduction-and-prerequisites.html#lecture-notes"><i class="fa fa-check"></i><b>1.2.2</b> Lecture notes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="section-2.-basics-of-bayesian-inferences.html"><a href="section-2.-basics-of-bayesian-inferences.html"><i class="fa fa-check"></i><b>2</b> Section 2. Basics of Bayesian inferences</a>
<ul>
<li class="chapter" data-level="2.1" data-path="section-2.-basics-of-bayesian-inferences.html"><a href="section-2.-basics-of-bayesian-inferences.html#resources-2"><i class="fa fa-check"></i><b>2.1</b> Resources</a></li>
<li class="chapter" data-level="2.2" data-path="section-2.-basics-of-bayesian-inferences.html"><a href="section-2.-basics-of-bayesian-inferences.html#notes-1"><i class="fa fa-check"></i><b>2.2</b> Notes</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="section-2.-basics-of-bayesian-inferences.html"><a href="section-2.-basics-of-bayesian-inferences.html#chapter-instructions"><i class="fa fa-check"></i><b>2.2.1</b> Chapter instructions</a></li>
<li class="chapter" data-level="2.2.2" data-path="section-2.-basics-of-bayesian-inferences.html"><a href="section-2.-basics-of-bayesian-inferences.html#chapter-2.-single-parameter-models"><i class="fa fa-check"></i><b>2.2.2</b> Chapter 2. Single-parameter models</a></li>
<li class="chapter" data-level="2.2.3" data-path="section-2.-basics-of-bayesian-inferences.html"><a href="section-2.-basics-of-bayesian-inferences.html#lecture-notes-1"><i class="fa fa-check"></i><b>2.2.3</b> Lecture notes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="section-3.-multidimensional-posterior.html"><a href="section-3.-multidimensional-posterior.html"><i class="fa fa-check"></i><b>3</b> Section 3. Multidimensional Posterior</a>
<ul>
<li class="chapter" data-level="3.1" data-path="section-3.-multidimensional-posterior.html"><a href="section-3.-multidimensional-posterior.html#resources-3"><i class="fa fa-check"></i><b>3.1</b> Resources</a></li>
<li class="chapter" data-level="3.2" data-path="section-3.-multidimensional-posterior.html"><a href="section-3.-multidimensional-posterior.html#notes-2"><i class="fa fa-check"></i><b>3.2</b> Notes</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="section-3.-multidimensional-posterior.html"><a href="section-3.-multidimensional-posterior.html#reading-instructions-1"><i class="fa fa-check"></i><b>3.2.1</b> Reading instructions</a></li>
<li class="chapter" data-level="3.2.2" data-path="section-3.-multidimensional-posterior.html"><a href="section-3.-multidimensional-posterior.html#chapter-3.-introduction-to-multiparameter-models"><i class="fa fa-check"></i><b>3.2.2</b> Chapter 3. Introduction to multiparameter models</a></li>
<li class="chapter" data-level="3.2.3" data-path="section-3.-multidimensional-posterior.html"><a href="section-3.-multidimensional-posterior.html#lecture-notes-2"><i class="fa fa-check"></i><b>3.2.3</b> Lecture notes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="section-4.-monte-carlo.html"><a href="section-4.-monte-carlo.html"><i class="fa fa-check"></i><b>4</b> Section 4. Monte Carlo</a>
<ul>
<li class="chapter" data-level="4.1" data-path="section-4.-monte-carlo.html"><a href="section-4.-monte-carlo.html#resources-4"><i class="fa fa-check"></i><b>4.1</b> Resources</a></li>
<li class="chapter" data-level="4.2" data-path="section-4.-monte-carlo.html"><a href="section-4.-monte-carlo.html#notes-3"><i class="fa fa-check"></i><b>4.2</b> Notes</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="section-4.-monte-carlo.html"><a href="section-4.-monte-carlo.html#reading-instructions-2"><i class="fa fa-check"></i><b>4.2.1</b> Reading instructions</a></li>
<li class="chapter" data-level="4.2.2" data-path="section-4.-monte-carlo.html"><a href="section-4.-monte-carlo.html#chapter-10.-introduction-to-bayesian-computation"><i class="fa fa-check"></i><b>4.2.2</b> Chapter 10. Introduction to Bayesian computation</a></li>
<li class="chapter" data-level="4.2.3" data-path="section-4.-monte-carlo.html"><a href="section-4.-monte-carlo.html#lecture-notes-3"><i class="fa fa-check"></i><b>4.2.3</b> Lecture notes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="section-5.-markov-chain-monte-carlo.html"><a href="section-5.-markov-chain-monte-carlo.html"><i class="fa fa-check"></i><b>5</b> Section 5. Markov chain Monte Carlo</a>
<ul>
<li class="chapter" data-level="5.1" data-path="section-5.-markov-chain-monte-carlo.html"><a href="section-5.-markov-chain-monte-carlo.html#resources-5"><i class="fa fa-check"></i><b>5.1</b> Resources</a></li>
<li class="chapter" data-level="5.2" data-path="section-5.-markov-chain-monte-carlo.html"><a href="section-5.-markov-chain-monte-carlo.html#notes-4"><i class="fa fa-check"></i><b>5.2</b> Notes</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="section-5.-markov-chain-monte-carlo.html"><a href="section-5.-markov-chain-monte-carlo.html#reading-instructions-3"><i class="fa fa-check"></i><b>5.2.1</b> Reading instructions</a></li>
<li class="chapter" data-level="5.2.2" data-path="section-5.-markov-chain-monte-carlo.html"><a href="section-5.-markov-chain-monte-carlo.html#chapter-11.-basics-of-markov-chain-simulation"><i class="fa fa-check"></i><b>5.2.2</b> Chapter 11. Basics of Markov chain simulation</a></li>
<li class="chapter" data-level="5.2.3" data-path="section-5.-markov-chain-monte-carlo.html"><a href="section-5.-markov-chain-monte-carlo.html#lecture-notes-4"><i class="fa fa-check"></i><b>5.2.3</b> Lecture notes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="section-6.-hmc-nuts-and-stan.html"><a href="section-6.-hmc-nuts-and-stan.html"><i class="fa fa-check"></i><b>6</b> Section 6. HMC, NUTS, and Stan</a>
<ul>
<li class="chapter" data-level="6.1" data-path="section-6.-hmc-nuts-and-stan.html"><a href="section-6.-hmc-nuts-and-stan.html#resources-6"><i class="fa fa-check"></i><b>6.1</b> Resources</a></li>
<li class="chapter" data-level="6.2" data-path="section-6.-hmc-nuts-and-stan.html"><a href="section-6.-hmc-nuts-and-stan.html#notes-5"><i class="fa fa-check"></i><b>6.2</b> Notes</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="section-6.-hmc-nuts-and-stan.html"><a href="section-6.-hmc-nuts-and-stan.html#reading-instructions-4"><i class="fa fa-check"></i><b>6.2.1</b> Reading instructions</a></li>
<li class="chapter" data-level="6.2.2" data-path="section-6.-hmc-nuts-and-stan.html"><a href="section-6.-hmc-nuts-and-stan.html#chapter-12.-computationally-efficient-markov-chain-simulation"><i class="fa fa-check"></i><b>6.2.2</b> Chapter 12. Computationally efficient Markov chain simulation</a></li>
<li class="chapter" data-level="6.2.3" data-path="section-6.-hmc-nuts-and-stan.html"><a href="section-6.-hmc-nuts-and-stan.html#lecture-notes-5"><i class="fa fa-check"></i><b>6.2.3</b> Lecture notes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="section-7.-hierarchical-models-and-exchangeability.html"><a href="section-7.-hierarchical-models-and-exchangeability.html"><i class="fa fa-check"></i><b>7</b> Section 7. Hierarchical models and exchangeability</a>
<ul>
<li class="chapter" data-level="7.1" data-path="section-7.-hierarchical-models-and-exchangeability.html"><a href="section-7.-hierarchical-models-and-exchangeability.html#resources-7"><i class="fa fa-check"></i><b>7.1</b> Resources</a></li>
<li class="chapter" data-level="7.2" data-path="section-7.-hierarchical-models-and-exchangeability.html"><a href="section-7.-hierarchical-models-and-exchangeability.html#notes-6"><i class="fa fa-check"></i><b>7.2</b> Notes</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="section-7.-hierarchical-models-and-exchangeability.html"><a href="section-7.-hierarchical-models-and-exchangeability.html#reading-instructions-5"><i class="fa fa-check"></i><b>7.2.1</b> Reading instructions</a></li>
<li class="chapter" data-level="7.2.2" data-path="section-7.-hierarchical-models-and-exchangeability.html"><a href="section-7.-hierarchical-models-and-exchangeability.html#chapter-5.-hierarchical-models"><i class="fa fa-check"></i><b>7.2.2</b> Chapter 5. Hierarchical models</a></li>
<li class="chapter" data-level="7.2.3" data-path="section-7.-hierarchical-models-and-exchangeability.html"><a href="section-7.-hierarchical-models-and-exchangeability.html#lecture-notes-6"><i class="fa fa-check"></i><b>7.2.3</b> Lecture notes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="section-8.-model-checking-cross-validation.html"><a href="section-8.-model-checking-cross-validation.html"><i class="fa fa-check"></i><b>8</b> Section 8. Model checking &amp; Cross-validation</a>
<ul>
<li class="chapter" data-level="8.1" data-path="section-8.-model-checking-cross-validation.html"><a href="section-8.-model-checking-cross-validation.html#resources-8"><i class="fa fa-check"></i><b>8.1</b> Resources</a></li>
<li class="chapter" data-level="8.2" data-path="section-8.-model-checking-cross-validation.html"><a href="section-8.-model-checking-cross-validation.html#notes-7"><i class="fa fa-check"></i><b>8.2</b> Notes</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="section-8.-model-checking-cross-validation.html"><a href="section-8.-model-checking-cross-validation.html#chapter-6-reading-instructions"><i class="fa fa-check"></i><b>8.2.1</b> Chapter 6 reading instructions</a></li>
<li class="chapter" data-level="8.2.2" data-path="section-8.-model-checking-cross-validation.html"><a href="section-8.-model-checking-cross-validation.html#chapter-6.-model-checking"><i class="fa fa-check"></i><b>8.2.2</b> Chapter 6. Model checking</a></li>
<li class="chapter" data-level="8.2.3" data-path="section-8.-model-checking-cross-validation.html"><a href="section-8.-model-checking-cross-validation.html#chapter-6.-lecture-notes"><i class="fa fa-check"></i><b>8.2.3</b> Chapter 6. Lecture notes</a></li>
<li class="chapter" data-level="8.2.4" data-path="section-8.-model-checking-cross-validation.html"><a href="section-8.-model-checking-cross-validation.html#chapter-7-reading-instructions"><i class="fa fa-check"></i><b>8.2.4</b> Chapter 7 reading instructions</a></li>
<li class="chapter" data-level="8.2.5" data-path="section-8.-model-checking-cross-validation.html"><a href="section-8.-model-checking-cross-validation.html#chapter-7.-evaluating-comparing-and-expanding-models"><i class="fa fa-check"></i><b>8.2.5</b> Chapter 7. Evaluating, comparing, and expanding models</a></li>
<li class="chapter" data-level="8.2.6" data-path="section-8.-model-checking-cross-validation.html"><a href="section-8.-model-checking-cross-validation.html#additional-reading"><i class="fa fa-check"></i><b>8.2.6</b> Additional Reading</a></li>
<li class="chapter" data-level="8.2.7" data-path="section-8.-model-checking-cross-validation.html"><a href="section-8.-model-checking-cross-validation.html#chapter-7.-lecture-notes"><i class="fa fa-check"></i><b>8.2.7</b> Chapter 7. Lecture notes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="section-9.-model-comparison-and-selection.html"><a href="section-9.-model-comparison-and-selection.html"><i class="fa fa-check"></i><b>9</b> Section 9. Model comparison and selection</a>
<ul>
<li class="chapter" data-level="9.1" data-path="section-9.-model-comparison-and-selection.html"><a href="section-9.-model-comparison-and-selection.html#resources-9"><i class="fa fa-check"></i><b>9.1</b> Resources</a></li>
<li class="chapter" data-level="9.2" data-path="section-9.-model-comparison-and-selection.html"><a href="section-9.-model-comparison-and-selection.html#notes-8"><i class="fa fa-check"></i><b>9.2</b> Notes</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="section-9.-model-comparison-and-selection.html"><a href="section-9.-model-comparison-and-selection.html#lecture-notes-7"><i class="fa fa-check"></i><b>9.2.1</b> Lecture notes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="section-10.-decision-analysis.html"><a href="section-10.-decision-analysis.html"><i class="fa fa-check"></i><b>10</b> Section 10. Decision analysis</a>
<ul>
<li class="chapter" data-level="10.1" data-path="section-10.-decision-analysis.html"><a href="section-10.-decision-analysis.html#resources-10"><i class="fa fa-check"></i><b>10.1</b> Resources</a></li>
<li class="chapter" data-level="10.2" data-path="section-10.-decision-analysis.html"><a href="section-10.-decision-analysis.html#notes-9"><i class="fa fa-check"></i><b>10.2</b> Notes</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="section-10.-decision-analysis.html"><a href="section-10.-decision-analysis.html#reading-instructions-6"><i class="fa fa-check"></i><b>10.2.1</b> Reading instructions</a></li>
<li class="chapter" data-level="10.2.2" data-path="section-10.-decision-analysis.html"><a href="section-10.-decision-analysis.html#chapter-9.-decision-analysis"><i class="fa fa-check"></i><b>10.2.2</b> Chapter 9. Decision analysis</a></li>
<li class="chapter" data-level="10.2.3" data-path="section-10.-decision-analysis.html"><a href="section-10.-decision-analysis.html#lecture-notes-8"><i class="fa fa-check"></i><b>10.2.3</b> Lecture notes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="section-11.-normal-approximation-frequency-properties.html"><a href="section-11.-normal-approximation-frequency-properties.html"><i class="fa fa-check"></i><b>11</b> Section 11. Normal approximation &amp; Frequency properties</a>
<ul>
<li class="chapter" data-level="11.1" data-path="section-11.-normal-approximation-frequency-properties.html"><a href="section-11.-normal-approximation-frequency-properties.html#resources-11"><i class="fa fa-check"></i><b>11.1</b> Resources</a></li>
<li class="chapter" data-level="11.2" data-path="section-11.-normal-approximation-frequency-properties.html"><a href="section-11.-normal-approximation-frequency-properties.html#notes-10"><i class="fa fa-check"></i><b>11.2</b> Notes</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="section-11.-normal-approximation-frequency-properties.html"><a href="section-11.-normal-approximation-frequency-properties.html#reading-instructions-7"><i class="fa fa-check"></i><b>11.2.1</b> Reading instructions</a></li>
<li class="chapter" data-level="11.2.2" data-path="section-11.-normal-approximation-frequency-properties.html"><a href="section-11.-normal-approximation-frequency-properties.html#chapter-4.-asymptotics-and-connections-to-non-bayesian-approaches"><i class="fa fa-check"></i><b>11.2.2</b> Chapter 4. Asymptotics and connections to non-Bayesian approaches</a></li>
<li class="chapter" data-level="11.2.3" data-path="section-11.-normal-approximation-frequency-properties.html"><a href="section-11.-normal-approximation-frequency-properties.html#lecture-notes-9"><i class="fa fa-check"></i><b>11.2.3</b> Lecture notes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="section-12.-extended-topics.html"><a href="section-12.-extended-topics.html"><i class="fa fa-check"></i><b>12</b> Section 12. Extended topics</a>
<ul>
<li class="chapter" data-level="12.1" data-path="section-12.-extended-topics.html"><a href="section-12.-extended-topics.html#resources-12"><i class="fa fa-check"></i><b>12.1</b> Resources</a></li>
<li class="chapter" data-level="12.2" data-path="section-12.-extended-topics.html"><a href="section-12.-extended-topics.html#notes-11"><i class="fa fa-check"></i><b>12.2</b> Notes</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="section-12.-extended-topics.html"><a href="section-12.-extended-topics.html#lecture-12.1-frequency-evaluation-hypothesis-testing-and-variable-selection"><i class="fa fa-check"></i><b>12.2.1</b> Lecture 12.1 Frequency evaluation, hypothesis testing and variable selection</a></li>
<li class="chapter" data-level="12.2.2" data-path="section-12.-extended-topics.html"><a href="section-12.-extended-topics.html#lecture-12.2-overview-of-modeling-data-collection-bda3-ch-8-linear-models-bda-ch-14-18-lasso-horseshoe-and-gaussian-processes-bda3-ch-21"><i class="fa fa-check"></i><b>12.2.2</b> Lecture 12.2 Overview of modeling data collection, BDA3 Ch 8, linear models, BDA Ch 14-18, lasso, horseshoe and Gaussian processes, BDA3 Ch 21</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="section-13.-notes-on-ch-14.-introduction-to-regression-models.html"><a href="section-13.-notes-on-ch-14.-introduction-to-regression-models.html"><i class="fa fa-check"></i><b>13</b> Section 13. Notes on ‘Ch 14. Introduction to regression models’</a>
<ul>
<li class="chapter" data-level="13.1" data-path="section-13.-notes-on-ch-14.-introduction-to-regression-models.html"><a href="section-13.-notes-on-ch-14.-introduction-to-regression-models.html#chapter-14.-introduction-to-regression-models"><i class="fa fa-check"></i><b>13.1</b> Chapter 14. Introduction to regression models</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="section-13.-notes-on-ch-14.-introduction-to-regression-models.html"><a href="section-13.-notes-on-ch-14.-introduction-to-regression-models.html#conditional-modeling"><i class="fa fa-check"></i><b>13.1.1</b> 14.1 Conditional modeling</a></li>
<li class="chapter" data-level="13.1.2" data-path="section-13.-notes-on-ch-14.-introduction-to-regression-models.html"><a href="section-13.-notes-on-ch-14.-introduction-to-regression-models.html#bayesian-analysis-of-classical-regression"><i class="fa fa-check"></i><b>13.1.2</b> 14.2 Bayesian analysis of classical regression</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="section-13.-notes-on-ch-14.-introduction-to-regression-models.html"><a href="section-13.-notes-on-ch-14.-introduction-to-regression-models.html#goals-of-regression-analysis"><i class="fa fa-check"></i><b>13.2</b> 14.4 Goals of regression analysis</a></li>
<li class="chapter" data-level="13.3" data-path="section-13.-notes-on-ch-14.-introduction-to-regression-models.html"><a href="section-13.-notes-on-ch-14.-introduction-to-regression-models.html#assembling-the-matrix-of-explanatory-variables"><i class="fa fa-check"></i><b>13.3</b> 14.5 Assembling the matrix of explanatory variables</a>
<ul>
<li class="chapter" data-level="13.3.1" data-path="section-13.-notes-on-ch-14.-introduction-to-regression-models.html"><a href="section-13.-notes-on-ch-14.-introduction-to-regression-models.html#identifiability-and-collinearity"><i class="fa fa-check"></i><b>13.3.1</b> Identifiability and collinearity</a></li>
<li class="chapter" data-level="13.3.2" data-path="section-13.-notes-on-ch-14.-introduction-to-regression-models.html"><a href="section-13.-notes-on-ch-14.-introduction-to-regression-models.html#nonlinear-relations"><i class="fa fa-check"></i><b>13.3.2</b> Nonlinear relations</a></li>
<li class="chapter" data-level="13.3.3" data-path="section-13.-notes-on-ch-14.-introduction-to-regression-models.html"><a href="section-13.-notes-on-ch-14.-introduction-to-regression-models.html#indicator-variables"><i class="fa fa-check"></i><b>13.3.3</b> Indicator variables</a></li>
<li class="chapter" data-level="13.3.4" data-path="section-13.-notes-on-ch-14.-introduction-to-regression-models.html"><a href="section-13.-notes-on-ch-14.-introduction-to-regression-models.html#interactions"><i class="fa fa-check"></i><b>13.3.4</b> Interactions</a></li>
</ul></li>
<li class="chapter" data-level="13.4" data-path="section-13.-notes-on-ch-14.-introduction-to-regression-models.html"><a href="section-13.-notes-on-ch-14.-introduction-to-regression-models.html#regularization-and-dimension-reduction"><i class="fa fa-check"></i><b>13.4</b> 14.6 Regularization and dimension reduction</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="section-14.-notes-on-ch-15.-hierarchical-linear-models.html"><a href="section-14.-notes-on-ch-15.-hierarchical-linear-models.html"><i class="fa fa-check"></i><b>14</b> Section 14. Notes on ‘Ch 15. Hierarchical linear models’</a>
<ul>
<li class="chapter" data-level="14.1" data-path="section-14.-notes-on-ch-15.-hierarchical-linear-models.html"><a href="section-14.-notes-on-ch-15.-hierarchical-linear-models.html#chapter-15.-hierarchical-linear-models"><i class="fa fa-check"></i><b>14.1</b> Chapter 15. Hierarchical linear models</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="section-17.-notes-on-ch-19.-parametric-nonlinear-models.html"><a href="section-17.-notes-on-ch-19.-parametric-nonlinear-models.html"><i class="fa fa-check"></i><b>15</b> Section 17. Notes on ‘Ch 19. Parametric nonlinear models’</a>
<ul>
<li class="chapter" data-level="15.1" data-path="section-17.-notes-on-ch-19.-parametric-nonlinear-models.html"><a href="section-17.-notes-on-ch-19.-parametric-nonlinear-models.html#chapter-19.-parametric-nonlinear-models"><i class="fa fa-check"></i><b>15.1</b> Chapter 19. Parametric nonlinear models</a>
<ul>
<li class="chapter" data-level="" data-path="section-17.-notes-on-ch-19.-parametric-nonlinear-models.html"><a href="section-17.-notes-on-ch-19.-parametric-nonlinear-models.html#example-serial-dilution-assay"><i class="fa fa-check"></i>19.1 Example: serial dilution assay</a></li>
<li class="chapter" data-level="" data-path="section-17.-notes-on-ch-19.-parametric-nonlinear-models.html"><a href="section-17.-notes-on-ch-19.-parametric-nonlinear-models.html#example-population-toxicokinetics"><i class="fa fa-check"></i>19.2 Example: population toxicokinetics</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="16" data-path="section-18.-notes-on-ch-20.-basis-function-models.html"><a href="section-18.-notes-on-ch-20.-basis-function-models.html"><i class="fa fa-check"></i><b>16</b> Section 18. Notes on ‘Ch 20. Basis function models’</a>
<ul>
<li class="chapter" data-level="16.1" data-path="section-18.-notes-on-ch-20.-basis-function-models.html"><a href="section-18.-notes-on-ch-20.-basis-function-models.html#chapter-20.-basis-function-models"><i class="fa fa-check"></i><b>16.1</b> Chapter 20. Basis function models</a>
<ul>
<li class="chapter" data-level="" data-path="section-18.-notes-on-ch-20.-basis-function-models.html"><a href="section-18.-notes-on-ch-20.-basis-function-models.html#splines-and-weighted-sums-of-basis-functions"><i class="fa fa-check"></i>20.1 Splines and weighted sums of basis functions</a></li>
</ul></li>
<li class="chapter" data-level="16.2" data-path="section-18.-notes-on-ch-20.-basis-function-models.html"><a href="section-18.-notes-on-ch-20.-basis-function-models.html#basis-selection-and-shrinkage-coefficients"><i class="fa fa-check"></i><b>16.2</b> 20.2 Basis selection and shrinkage coefficients</a>
<ul>
<li class="chapter" data-level="" data-path="section-18.-notes-on-ch-20.-basis-function-models.html"><a href="section-18.-notes-on-ch-20.-basis-function-models.html#shrinkage-priors"><i class="fa fa-check"></i>Shrinkage priors</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="section-18.-notes-on-ch-20.-basis-function-models.html"><a href="section-18.-notes-on-ch-20.-basis-function-models.html#non-normal-models-and-regression-surfaces"><i class="fa fa-check"></i><b>16.3</b> 20.3 Non-normal models and regression surfaces</a>
<ul>
<li class="chapter" data-level="" data-path="section-18.-notes-on-ch-20.-basis-function-models.html"><a href="section-18.-notes-on-ch-20.-basis-function-models.html#other-error-distributions"><i class="fa fa-check"></i>Other error distributions</a></li>
<li class="chapter" data-level="" data-path="section-18.-notes-on-ch-20.-basis-function-models.html"><a href="section-18.-notes-on-ch-20.-basis-function-models.html#multivariate-regression-surfaces"><i class="fa fa-check"></i>Multivariate regression surfaces</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="section-19.-notes-on-ch-21.-gaussian-process-models.html"><a href="section-19.-notes-on-ch-21.-gaussian-process-models.html"><i class="fa fa-check"></i><b>17</b> Section 19. Notes on ‘Ch 21. Gaussian process models’</a>
<ul>
<li class="chapter" data-level="17.1" data-path="section-19.-notes-on-ch-21.-gaussian-process-models.html"><a href="section-19.-notes-on-ch-21.-gaussian-process-models.html#chapter-21.-gaussian-process-models"><i class="fa fa-check"></i><b>17.1</b> Chapter 21. Gaussian process models</a>
<ul>
<li class="chapter" data-level="" data-path="section-19.-notes-on-ch-21.-gaussian-process-models.html"><a href="section-19.-notes-on-ch-21.-gaussian-process-models.html#gaussian-process-regression"><i class="fa fa-check"></i>21.1 Gaussian process regression</a></li>
<li class="chapter" data-level="" data-path="section-19.-notes-on-ch-21.-gaussian-process-models.html"><a href="section-19.-notes-on-ch-21.-gaussian-process-models.html#latent-gaussian-process-models"><i class="fa fa-check"></i>21.3 Latent Gaussian process models</a></li>
<li class="chapter" data-level="" data-path="section-19.-notes-on-ch-21.-gaussian-process-models.html"><a href="section-19.-notes-on-ch-21.-gaussian-process-models.html#functional-data-analysis"><i class="fa fa-check"></i>21.4 Functional data analysis</a></li>
<li class="chapter" data-level="" data-path="section-19.-notes-on-ch-21.-gaussian-process-models.html"><a href="section-19.-notes-on-ch-21.-gaussian-process-models.html#density-estimation-and-regression"><i class="fa fa-check"></i>21.5 Density estimation and regression</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="section-20.-notes-on-ch-22.-finite-mixture-models.html"><a href="section-20.-notes-on-ch-22.-finite-mixture-models.html"><i class="fa fa-check"></i><b>18</b> Section 20. Notes on ‘Ch 22. Finite mixture models’</a>
<ul>
<li class="chapter" data-level="18.1" data-path="section-20.-notes-on-ch-22.-finite-mixture-models.html"><a href="section-20.-notes-on-ch-22.-finite-mixture-models.html#chapter-22.-finite-mixture-models"><i class="fa fa-check"></i><b>18.1</b> Chapter 22. Finite mixture models</a>
<ul>
<li class="chapter" data-level="" data-path="section-20.-notes-on-ch-22.-finite-mixture-models.html"><a href="section-20.-notes-on-ch-22.-finite-mixture-models.html#setting-up-and-interpreting-mixture-models"><i class="fa fa-check"></i>22.1 Setting up and interpreting mixture models</a></li>
<li class="chapter" data-level="" data-path="section-20.-notes-on-ch-22.-finite-mixture-models.html"><a href="section-20.-notes-on-ch-22.-finite-mixture-models.html#unspecifed-number-of-mixture-components"><i class="fa fa-check"></i>22.4 Unspecifed number of mixture components</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="19" data-path="section-21.-notes-on-ch-23.-dirichlet-process-models.html"><a href="section-21.-notes-on-ch-23.-dirichlet-process-models.html"><i class="fa fa-check"></i><b>19</b> Section 21. Notes on ‘Ch 23. Dirichlet process models’</a>
<ul>
<li class="chapter" data-level="19.1" data-path="section-21.-notes-on-ch-23.-dirichlet-process-models.html"><a href="section-21.-notes-on-ch-23.-dirichlet-process-models.html#chapter-23.-dirichlet-process-models"><i class="fa fa-check"></i><b>19.1</b> Chapter 23. Dirichlet process models</a>
<ul>
<li class="chapter" data-level="" data-path="section-21.-notes-on-ch-23.-dirichlet-process-models.html"><a href="section-21.-notes-on-ch-23.-dirichlet-process-models.html#bayesian-histograms"><i class="fa fa-check"></i>23.1 Bayesian histograms</a></li>
<li class="chapter" data-level="" data-path="section-21.-notes-on-ch-23.-dirichlet-process-models.html"><a href="section-21.-notes-on-ch-23.-dirichlet-process-models.html#dirichlet-process-prior-distributions"><i class="fa fa-check"></i>23.2 Dirichlet process prior distributions</a></li>
<li class="chapter" data-level="" data-path="section-21.-notes-on-ch-23.-dirichlet-process-models.html"><a href="section-21.-notes-on-ch-23.-dirichlet-process-models.html#dirichlet-process-mixtures"><i class="fa fa-check"></i>23.3 Dirichlet process mixtures</a></li>
<li class="chapter" data-level="" data-path="section-21.-notes-on-ch-23.-dirichlet-process-models.html"><a href="section-21.-notes-on-ch-23.-dirichlet-process-models.html#beyond-density-estimation"><i class="fa fa-check"></i>23.4 Beyond density estimation</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Assignments</b></span></li>
<li class="chapter" data-level="" data-path="assignments-intro.html"><a href="assignments-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="20" data-path="assignment-1.html"><a href="assignment-1.html"><i class="fa fa-check"></i><b>20</b> Assignment 1</a>
<ul>
<li class="chapter" data-level="20.1" data-path="assignment-1.html"><a href="assignment-1.html#setup"><i class="fa fa-check"></i><b>20.1</b> Setup</a></li>
<li class="chapter" data-level="20.2" data-path="assignment-1.html"><a href="assignment-1.html#exercise-1"><i class="fa fa-check"></i><b>20.2</b> Exercise 1</a></li>
<li class="chapter" data-level="20.3" data-path="assignment-1.html"><a href="assignment-1.html#exercise-3"><i class="fa fa-check"></i><b>20.3</b> Exercise 3</a></li>
<li class="chapter" data-level="20.4" data-path="assignment-1.html"><a href="assignment-1.html#exercise-4"><i class="fa fa-check"></i><b>20.4</b> Exercise 4</a>
<ul>
<li class="chapter" data-level="20.4.1" data-path="assignment-1.html"><a href="assignment-1.html#a"><i class="fa fa-check"></i><b>20.4.1</b> 4.a</a></li>
</ul></li>
<li class="chapter" data-level="20.5" data-path="assignment-1.html"><a href="assignment-1.html#exercise-5"><i class="fa fa-check"></i><b>20.5</b> Exercise 5</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="assignment-2.html"><a href="assignment-2.html"><i class="fa fa-check"></i><b>21</b> Assignment 2</a>
<ul>
<li class="chapter" data-level="21.1" data-path="assignment-2.html"><a href="assignment-2.html#setup-1"><i class="fa fa-check"></i><b>21.1</b> Setup</a></li>
<li class="chapter" data-level="21.2" data-path="assignment-2.html"><a href="assignment-2.html#exercise-1.-inference-for-binomial-proportion"><i class="fa fa-check"></i><b>21.2</b> Exercise 1. Inference for binomial proportion</a></li>
</ul></li>
<li class="chapter" data-level="22" data-path="assignment-3.html"><a href="assignment-3.html"><i class="fa fa-check"></i><b>22</b> Assignment 3</a>
<ul>
<li class="chapter" data-level="22.1" data-path="assignment-3.html"><a href="assignment-3.html#setup-2"><i class="fa fa-check"></i><b>22.1</b> Setup</a></li>
<li class="chapter" data-level="22.2" data-path="assignment-3.html"><a href="assignment-3.html#exercise-1.-inference-for-normal-mean-and-deviation"><i class="fa fa-check"></i><b>22.2</b> Exercise 1. Inference for normal mean and deviation</a></li>
<li class="chapter" data-level="22.3" data-path="assignment-3.html"><a href="assignment-3.html#exercise-2.-inference-for-the-difference-between-proportions"><i class="fa fa-check"></i><b>22.3</b> Exercise 2. Inference for the difference between proportions</a></li>
<li class="chapter" data-level="22.4" data-path="assignment-3.html"><a href="assignment-3.html#exercise-3.-inference-for-the-difference-between-normal-means"><i class="fa fa-check"></i><b>22.4</b> Exercise 3. Inference for the difference between normal means</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="assignment-4.html"><a href="assignment-4.html"><i class="fa fa-check"></i><b>23</b> Assignment 4</a>
<ul>
<li class="chapter" data-level="23.1" data-path="assignment-4.html"><a href="assignment-4.html#setup-3"><i class="fa fa-check"></i><b>23.1</b> Setup</a></li>
<li class="chapter" data-level="23.2" data-path="assignment-4.html"><a href="assignment-4.html#exercise-1.-bioassay-model"><i class="fa fa-check"></i><b>23.2</b> Exercise 1. Bioassay model</a></li>
</ul></li>
<li class="chapter" data-level="24" data-path="assignment-5.html"><a href="assignment-5.html"><i class="fa fa-check"></i><b>24</b> Assignment 5</a>
<ul>
<li class="chapter" data-level="24.1" data-path="assignment-5.html"><a href="assignment-5.html#setup-4"><i class="fa fa-check"></i><b>24.1</b> Setup</a></li>
<li class="chapter" data-level="24.2" data-path="assignment-5.html"><a href="assignment-5.html#generalized-linear-model-bioassay-with-metropolis"><i class="fa fa-check"></i><b>24.2</b> Generalized linear model: Bioassay with Metropolis</a>
<ul>
<li class="chapter" data-level="24.2.1" data-path="assignment-5.html"><a href="assignment-5.html#exercise-1."><i class="fa fa-check"></i><b>24.2.1</b> Exercise 1.</a></li>
<li class="chapter" data-level="24.2.2" data-path="assignment-5.html"><a href="assignment-5.html#exercise-2."><i class="fa fa-check"></i><b>24.2.2</b> Exercise 2.</a></li>
<li class="chapter" data-level="24.2.3" data-path="assignment-5.html"><a href="assignment-5.html#exercise-3-1"><i class="fa fa-check"></i><b>24.2.3</b> Exercise 3</a></li>
<li class="chapter" data-level="24.2.4" data-path="assignment-5.html"><a href="assignment-5.html#exercise-4-1"><i class="fa fa-check"></i><b>24.2.4</b> Exercise 4</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="25" data-path="assignment-6.html"><a href="assignment-6.html"><i class="fa fa-check"></i><b>25</b> Assignment 6</a>
<ul>
<li class="chapter" data-level="25.1" data-path="assignment-6.html"><a href="assignment-6.html#setup-5"><i class="fa fa-check"></i><b>25.1</b> Setup</a></li>
<li class="chapter" data-level="25.2" data-path="assignment-6.html"><a href="assignment-6.html#exercise-1.-generalized-linear-model-bioassay-with-stan"><i class="fa fa-check"></i><b>25.2</b> Exercise 1. Generalized linear model: Bioassay with Stan</a>
<ul>
<li class="chapter" data-level="25.2.1" data-path="assignment-6.html"><a href="assignment-6.html#write-down-the-model-for-the-bioassay-data-in-stan-syntax."><i class="fa fa-check"></i><b>25.2.1</b> 1. Write down the model for the bioassay data in Stan syntax.</a></li>
<li class="chapter" data-level="25.2.2" data-path="assignment-6.html"><a href="assignment-6.html#use-widehatr-for-convergence-analysis."><i class="fa fa-check"></i><b>25.2.2</b> 2. Use <span class="math inline">\(\widehat{R}\)</span> for convergence analysis.</a></li>
<li class="chapter" data-level="25.2.3" data-path="assignment-6.html"><a href="assignment-6.html#plot-the-draws-for-alpha-and-beta-scatter-plot-and-include-this-plot-in-your-report"><i class="fa fa-check"></i><b>25.2.3</b> 3. Plot the draws for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> (scatter plot) and include this plot in your report</a></li>
<li class="chapter" data-level="25.2.4" data-path="assignment-6.html"><a href="assignment-6.html#to-develop-the-course-and-provide-feedback-to-stan-developers-we-collect-information-on-which-stan-setup-you-used-and-whether-you-had-any-problems-in-setting-it-up-or-using-it."><i class="fa fa-check"></i><b>25.2.4</b> 4. To develop the course and provide feedback to Stan developers, we collect information on which Stan setup you used and whether you had any problems in setting it up or using it.</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="26" data-path="assignment-7.html"><a href="assignment-7.html"><i class="fa fa-check"></i><b>26</b> Assignment 7</a>
<ul>
<li class="chapter" data-level="26.1" data-path="assignment-7.html"><a href="assignment-7.html#setup-6"><i class="fa fa-check"></i><b>26.1</b> Setup</a></li>
<li class="chapter" data-level="26.2" data-path="assignment-7.html"><a href="assignment-7.html#linear-model-drowning-data-with-stan"><i class="fa fa-check"></i><b>26.2</b> 1. Linear model: drowning data with Stan</a></li>
<li class="chapter" data-level="26.3" data-path="assignment-7.html"><a href="assignment-7.html#hierarchical-model-factory-data-with-stan"><i class="fa fa-check"></i><b>26.3</b> 2. Hierarchical model: factory data with Stan</a></li>
</ul></li>
<li class="chapter" data-level="27" data-path="assignment-8.html"><a href="assignment-8.html"><i class="fa fa-check"></i><b>27</b> Assignment 8</a>
<ul>
<li class="chapter" data-level="27.1" data-path="assignment-8.html"><a href="assignment-8.html#setup-7"><i class="fa fa-check"></i><b>27.1</b> Setup</a></li>
<li class="chapter" data-level="27.2" data-path="assignment-8.html"><a href="assignment-8.html#exercise-1.-model-assessment-loo-cv-for-factory-data-with-stan"><i class="fa fa-check"></i><b>27.2</b> Exercise 1. Model assessment: LOO-CV for factory data with Stan</a></li>
</ul></li>
<li class="chapter" data-level="28" data-path="assignment-9.html"><a href="assignment-9.html"><i class="fa fa-check"></i><b>28</b> Assignment 9</a>
<ul>
<li class="chapter" data-level="28.1" data-path="assignment-9.html"><a href="assignment-9.html#setup-8"><i class="fa fa-check"></i><b>28.1</b> Setup</a></li>
<li class="chapter" data-level="28.2" data-path="assignment-9.html"><a href="assignment-9.html#exercise-1.-decision-analysis-for-the-factory-data"><i class="fa fa-check"></i><b>28.2</b> Exercise 1. Decision analysis for the factory data</a></li>
</ul></li>
<li class="part"><span><b>III Book Exercises</b></span></li>
<li class="chapter" data-level="" data-path="exercises-intro.html"><a href="exercises-intro.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="29" data-path="chapter-1-exercises.html"><a href="chapter-1-exercises.html"><i class="fa fa-check"></i><b>29</b> Chapter 1 Exercises</a>
<ul>
<li class="chapter" data-level="29.1" data-path="chapter-1-exercises.html"><a href="chapter-1-exercises.html#question-1"><i class="fa fa-check"></i><b>29.1</b> Question 1</a></li>
<li class="chapter" data-level="29.2" data-path="chapter-1-exercises.html"><a href="chapter-1-exercises.html#question-2"><i class="fa fa-check"></i><b>29.2</b> Question 2</a></li>
<li class="chapter" data-level="29.3" data-path="chapter-1-exercises.html"><a href="chapter-1-exercises.html#question-6"><i class="fa fa-check"></i><b>29.3</b> Question 6</a></li>
<li class="chapter" data-level="29.4" data-path="chapter-1-exercises.html"><a href="chapter-1-exercises.html#question-8"><i class="fa fa-check"></i><b>29.4</b> Question 8</a></li>
</ul></li>
<li class="chapter" data-level="30" data-path="chapter-2-exercises.html"><a href="chapter-2-exercises.html"><i class="fa fa-check"></i><b>30</b> Chapter 2 Exercises</a>
<ul>
<li class="chapter" data-level="30.1" data-path="chapter-2-exercises.html"><a href="chapter-2-exercises.html#setup-9"><i class="fa fa-check"></i><b>30.1</b> Setup</a></li>
<li class="chapter" data-level="30.2" data-path="chapter-2-exercises.html"><a href="chapter-2-exercises.html#question-1-1"><i class="fa fa-check"></i><b>30.2</b> Question 1</a></li>
<li class="chapter" data-level="30.3" data-path="chapter-2-exercises.html"><a href="chapter-2-exercises.html#question-2-1"><i class="fa fa-check"></i><b>30.3</b> Question 2</a></li>
<li class="chapter" data-level="30.4" data-path="chapter-2-exercises.html"><a href="chapter-2-exercises.html#question-3"><i class="fa fa-check"></i><b>30.4</b> Question 3</a></li>
<li class="chapter" data-level="30.5" data-path="chapter-2-exercises.html"><a href="chapter-2-exercises.html#question-4"><i class="fa fa-check"></i><b>30.5</b> Question 4</a></li>
</ul></li>
<li class="chapter" data-level="31" data-path="chapter-5-exercises.html"><a href="chapter-5-exercises.html"><i class="fa fa-check"></i><b>31</b> Chapter 5 Exercises</a>
<ul>
<li class="chapter" data-level="31.1" data-path="chapter-5-exercises.html"><a href="chapter-5-exercises.html#question-1-2"><i class="fa fa-check"></i><b>31.1</b> Question 1</a></li>
</ul></li>
<li class="chapter" data-level="32" data-path="chapter-19-exercises---reproducing-the-the-serial-dilution-assay.html"><a href="chapter-19-exercises---reproducing-the-the-serial-dilution-assay.html"><i class="fa fa-check"></i><b>32</b> Chapter 19 Exercises - Reproducing the the ‘serial dilution assay’</a>
<ul>
<li class="chapter" data-level="32.1" data-path="chapter-19-exercises---reproducing-the-the-serial-dilution-assay.html"><a href="chapter-19-exercises---reproducing-the-the-serial-dilution-assay.html#setup-10"><i class="fa fa-check"></i><b>32.1</b> Setup</a></li>
<li class="chapter" data-level="32.2" data-path="chapter-19-exercises---reproducing-the-the-serial-dilution-assay.html"><a href="chapter-19-exercises---reproducing-the-the-serial-dilution-assay.html#modeling"><i class="fa fa-check"></i><b>32.2</b> Modeling</a>
<ul>
<li class="chapter" data-level="32.2.1" data-path="chapter-19-exercises---reproducing-the-the-serial-dilution-assay.html"><a href="chapter-19-exercises---reproducing-the-the-serial-dilution-assay.html#model-specification"><i class="fa fa-check"></i><b>32.2.1</b> Model specification</a></li>
<li class="chapter" data-level="32.2.2" data-path="chapter-19-exercises---reproducing-the-the-serial-dilution-assay.html"><a href="chapter-19-exercises---reproducing-the-the-serial-dilution-assay.html#in-stan"><i class="fa fa-check"></i><b>32.2.2</b> In Stan</a></li>
<li class="chapter" data-level="32.2.3" data-path="chapter-19-exercises---reproducing-the-the-serial-dilution-assay.html"><a href="chapter-19-exercises---reproducing-the-the-serial-dilution-assay.html#sampling"><i class="fa fa-check"></i><b>32.2.3</b> Sampling</a></li>
<li class="chapter" data-level="32.2.4" data-path="chapter-19-exercises---reproducing-the-the-serial-dilution-assay.html"><a href="chapter-19-exercises---reproducing-the-the-serial-dilution-assay.html#posterior-distributions"><i class="fa fa-check"></i><b>32.2.4</b> Posterior distributions</a></li>
<li class="chapter" data-level="32.2.5" data-path="chapter-19-exercises---reproducing-the-the-serial-dilution-assay.html"><a href="chapter-19-exercises---reproducing-the-the-serial-dilution-assay.html#posterior-predictive-check"><i class="fa fa-check"></i><b>32.2.5</b> Posterior predictive check</a></li>
</ul></li>
<li class="chapter" data-level="32.3" data-path="chapter-19-exercises---reproducing-the-the-serial-dilution-assay.html"><a href="chapter-19-exercises---reproducing-the-the-serial-dilution-assay.html#session-info"><i class="fa fa-check"></i><b>32.3</b> Session info</a></li>
</ul></li>
<li class="part"><span><b>IV Models</b></span></li>
<li class="chapter" data-level="33" data-path="stan-models-1.html"><a href="stan-models-1.html"><i class="fa fa-check"></i><b>33</b> Stan models</a>
<ul>
<li class="chapter" data-level="33.1" data-path="stan-models-1.html"><a href="stan-models-1.html#model-8-schools.stan"><i class="fa fa-check"></i><b>33.1</b> Model: <code>8-schools.stan</code></a></li>
<li class="chapter" data-level="33.2" data-path="stan-models-1.html"><a href="stan-models-1.html#model-assignment06-bioassay.stan"><i class="fa fa-check"></i><b>33.2</b> Model: <code>assignment06-bioassay.stan</code></a></li>
<li class="chapter" data-level="33.3" data-path="stan-models-1.html"><a href="stan-models-1.html#model-assignment07_factories_hierarchical.stan"><i class="fa fa-check"></i><b>33.3</b> Model: <code>assignment07_factories_hierarchical.stan</code></a></li>
<li class="chapter" data-level="33.4" data-path="stan-models-1.html"><a href="stan-models-1.html#model-assignment07_factories_pooled.stan"><i class="fa fa-check"></i><b>33.4</b> Model: <code>assignment07_factories_pooled.stan</code></a></li>
<li class="chapter" data-level="33.5" data-path="stan-models-1.html"><a href="stan-models-1.html#model-assignment07_factories_separate.stan"><i class="fa fa-check"></i><b>33.5</b> Model: <code>assignment07_factories_separate.stan</code></a></li>
<li class="chapter" data-level="33.6" data-path="stan-models-1.html"><a href="stan-models-1.html#model-assignment07-drownings.stan"><i class="fa fa-check"></i><b>33.6</b> Model: <code>assignment07-drownings.stan</code></a></li>
<li class="chapter" data-level="33.7" data-path="stan-models-1.html"><a href="stan-models-1.html#model-serial-dilution.stan"><i class="fa fa-check"></i><b>33.7</b> Model: <code>serial-dilution.stan</code></a></li>
</ul></li>
<li class="part"><span><b>V Misc</b></span></li>
<li class="chapter" data-level="" data-path="about.html"><a href="about.html"><i class="fa fa-check"></i>About</a>
<ul>
<li class="chapter" data-level="33.8" data-path="about.html"><a href="about.html#the-website"><i class="fa fa-check"></i><b>33.8</b> The website</a></li>
<li class="chapter" data-level="33.9" data-path="about.html"><a href="about.html#about-me"><i class="fa fa-check"></i><b>33.9</b> About me</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./"></a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="section-8.-model-checking-cross-validation" class="section level1" number="8">
<h1><span class="header-section-number">8</span> Section 8. Model checking &amp; Cross-validation</h1>
<p>2021-10-31</p>
<div id="resources-8" class="section level2" number="8.1">
<h2><span class="header-section-number">8.1</span> Resources</h2>
<ul>
<li>reading:
<ul>
<li>BDA3 ch. 6 “Model checking” and <a href="https://github.com/jhrcook/bayesian-data-analysis-course/tree/master/course-material/BDA3_ch06_reading-instructions.pdf">ch. 6 reading instructions</a></li>
<li>BDA3 ch. 7 “Evaluating, comparing, and expanding models” and <a href="../reading-instructions/BDA3_ch07_reading-instructions.pdf">ch. 7 reading instructions</a></li>
<li>read <em>Visualization in Bayesian workflow</em> (<a href="https://github.com/jhrcook/bayesian-data-analysis-course/tree/master/reading/Visualization-in-Bayesian-workflow.pdf">pdf</a>, <a href="https://doi.org/10.1111/rssa.12378">link</a>)</li>
<li>read <em>Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC</em> (<a href="https://github.com/jhrcook/bayesian-data-analysis-course/tree/master/course-material/Practical-Bayesian-model-evaluation-using-leave-one-out-cross-validation-and-WAIC.pdf">pdf</a>, <a href="https://arxiv.org/abs/1507.04544">link</a>)</li>
<li>additional reading material:
<ul>
<li><a href="https://avehtari.github.io/modelselection/">Model assesment, selection and inference after selection</a></li>
<li><a href="https://avehtari.github.io/modelselection/CV-FAQ.html">Cross-validation FAQ</a></li>
</ul></li>
</ul></li>
<li>lectures:
<ul>
<li><a href="https://aalto.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=7047e366-0df6-453c-867f-aafb00ca2d78">‘Lecture 8.1. Model Checking’</a></li>
<li><a href="https://aalto.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=d7849131-0afd-4ae6-ad64-aafb00da36f4">‘Lecture 8.2. Cross-Validation (part 1)’</a></li>
</ul></li>
<li>slides:
<ul>
<li><a href="https://github.com/jhrcook/bayesian-data-analysis-course/tree/master/course-material/slides_ch6.pdf">ch 6 slides</a></li>
<li><a href="https://github.com/jhrcook/bayesian-data-analysis-course/tree/master/course-material/slides_ch7.pdf">ch 7 slides</a></li>
</ul></li>
</ul>
</div>
<div id="notes-7" class="section level2" number="8.2">
<h2><span class="header-section-number">8.2</span> Notes</h2>
<div id="chapter-6-reading-instructions" class="section level3" number="8.2.1">
<h3><span class="header-section-number">8.2.1</span> Chapter 6 reading instructions</h3>
<ul>
<li>Replicates vs. future observation
<ul>
<li>predictive <span class="math inline">\(\tilde{y}\)</span>: the next yet unobserved possible observation</li>
<li><span class="math inline">\(y^\text{rep}\)</span>: replicating the whole experiment (with same values of <span class="math inline">\(x\)</span>) and obtaining as many replicated observations as in the original data</li>
</ul></li>
<li>Posterior predictive <em>p</em>-values
<ul>
<li><strong>do not recommend <em>p</em>-values any more</strong> especially in a form of hypothesis testing</li>
</ul></li>
<li>Prior predictive checking
<ul>
<li>using just the prior predictive distributions for assessing the sensibility of the model and priors before observing any data</li>
</ul></li>
</ul>
</div>
<div id="chapter-6.-model-checking" class="section level3" number="8.2.2">
<h3><span class="header-section-number">8.2.2</span> Chapter 6. Model checking</h3>
<div id="the-place-of-model-checking-in-applied-bayesian-statistics" class="section level4 unnumbered">
<h4>6.1 The place of model checking in applied Bayesian statistics</h4>
<ul>
<li>must assess the fit of a model to the data and to our substantive domain knowledge</li>
</ul>
<div id="sensitivity-analysis-and-model-improvement" class="section level5 unnumbered">
<h5>Sensitivity analysis and model improvement</h5>
<ul>
<li><em>sensitivity analysis</em>: “how much do posterior inference change when other reasonable probability models are used in place of the present model?” (pg. 141)</li>
</ul>
</div>
<div id="judging-model-flaws-by-their-practical-implications" class="section level5 unnumbered">
<h5>Judging model flaws by their practical implications</h5>
<ul>
<li>not interested in if the model is true or false - will likely always be false</li>
<li>more interested in the question: “Do the model’s deficiencies have a noticeable effect on the substantive inferences?” (pg. 142)
<ul>
<li>keep focus on the more important parts of the model, too</li>
</ul></li>
</ul>
</div>
</div>
<div id="do-the-inferences-from-the-model-make-sense" class="section level4 unnumbered">
<h4>6.2 Do the inferences from the model make sense?</h4>
<ul>
<li>there will be knowledge that is not included in the model
<ul>
<li>if the additional information suggests that posterior inferences are false, this suggests an option for improving the model’s accuracy</li>
</ul></li>
</ul>
<div id="external-validation" class="section level5 unnumbered">
<h5>External validation</h5>
<ul>
<li><em>external validation</em>: “using the model to make predictions about future data, and then collecting those data and comparing to their predictions” (pg. 142)</li>
</ul>
</div>
</div>
<div id="posterior-predictive-checking" class="section level4 unnumbered">
<h4>6.3 Posterior predictive checking</h4>
<ul>
<li>if the model fits, then generated replicate data should look like the observed data
<ul>
<li>“the observed data should look plausible under the posterior predictive distribution” (pg. 143)</li>
<li>is a <em>self-consistency check</em></li>
</ul></li>
<li>important to choose test quantities that are of interest of the goals of the model
<ul>
<li>may be inaccurate in some regards, but the relevance should be taken into account</li>
</ul></li>
<li>need not worry about adjusting for multiple comparisons:
<ul>
<li>“We are not concerned with ‘Type I error’ rate… because we use the checks not to accept or reject a model but rather to understand the limits of its applicability in realistic replications.” (pg. 150)</li>
</ul></li>
</ul>
</div>
<div id="graphical-posterior-predictive-checks" class="section level4 unnumbered">
<h4>6.4 Graphical posterior predictive checks</h4>
<ul>
<li>three types of graphical display to start a posterior predictive check:
<ol style="list-style-type: decimal">
<li>direct display of all the data</li>
</ol>
<ul>
<li>may need to get clever with how to do this effectively</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>display of data summaries or parameter inferences</li>
</ol>
<ul>
<li>useful when dataset is very large or want to focus on a particular part of the model</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li>graphs of residuals or other measures of discrepancy between the model and data</li>
</ol>
<ul>
<li>description of how to do this effectively for discrete data</li>
</ul></li>
</ul>
</div>
<div id="model-checking-for-the-educational-testing-example" class="section level4 unnumbered">
<h4>6.5 Model checking for the educational testing example</h4>
<ul>
<li>check that posterior parameter values and predictions are reasonable</li>
<li>compare summary statistics of real data and predictive distributions
<ul>
<li>min. and max. values, averages, skewness</li>
</ul></li>
<li>sensitivity analysis can assuage concerns that the outcome was not determined by specific choices of priors</li>
</ul>
</div>
</div>
<div id="chapter-6.-lecture-notes" class="section level3" number="8.2.3">
<h3><span class="header-section-number">8.2.3</span> Chapter 6. Lecture notes</h3>
<div id="lecture-8.1.-model-checking" class="section level4 unnumbered">
<h4>Lecture 8.1. Model Checking</h4>
<ul>
<li>model checking overview:
<ul>
<li>Sensibility with respect to additional information not used in modeling
<ul>
<li>e.g., if posterior would claim that hazardous chemical decreases probability of death</li>
</ul></li>
<li>External validation
<ul>
<li>compare predictions to completely new observations</li>
<li>compare to theoretical values
<ul>
<li>e.g., relativity theory predictions on the speed of light (not based on model optimized to data)</li>
</ul></li>
</ul></li>
<li>Internal validation
<ul>
<li>posterior predictive checking</li>
<li>cross-validation predictive checking</li>
</ul></li>
</ul></li>
<li>example of posterior checks with air quality model</li>
<li>examples with binary data and logistic regression</li>
<li>get posterior predictive distribution in Stan:</li>
</ul>
<div class="sourceCode" id="cb27"><pre class="sourceCode stan"><code class="sourceCode stan"><span id="cb27-1"><a href="section-8.-model-checking-cross-validation.html#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="kw">data</span> {</span>
<span id="cb27-2"><a href="section-8.-model-checking-cross-validation.html#cb27-2" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">1</span>&gt; N;</span>
<span id="cb27-3"><a href="section-8.-model-checking-cross-validation.html#cb27-3" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; y[N];</span>
<span id="cb27-4"><a href="section-8.-model-checking-cross-validation.html#cb27-4" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb27-5"><a href="section-8.-model-checking-cross-validation.html#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="kw">parameters</span> {</span>
<span id="cb27-6"><a href="section-8.-model-checking-cross-validation.html#cb27-6" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span>&lt;<span class="kw">lower</span>=<span class="dv">0</span>&gt; lambda;</span>
<span id="cb27-7"><a href="section-8.-model-checking-cross-validation.html#cb27-7" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb27-8"><a href="section-8.-model-checking-cross-validation.html#cb27-8" aria-hidden="true" tabindex="-1"></a><span class="kw">model</span> {</span>
<span id="cb27-9"><a href="section-8.-model-checking-cross-validation.html#cb27-9" aria-hidden="true" tabindex="-1"></a>  lambda ~ exponential(<span class="fl">0.2</span>);</span>
<span id="cb27-10"><a href="section-8.-model-checking-cross-validation.html#cb27-10" aria-hidden="true" tabindex="-1"></a>  y ~ poisson(lambda);</span>
<span id="cb27-11"><a href="section-8.-model-checking-cross-validation.html#cb27-11" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb27-12"><a href="section-8.-model-checking-cross-validation.html#cb27-12" aria-hidden="true" tabindex="-1"></a>generated_quantities {</span>
<span id="cb27-13"><a href="section-8.-model-checking-cross-validation.html#cb27-13" aria-hidden="true" tabindex="-1"></a>  <span class="dt">real</span> log_lik[N];</span>
<span id="cb27-14"><a href="section-8.-model-checking-cross-validation.html#cb27-14" aria-hidden="true" tabindex="-1"></a>  <span class="dt">int</span> y_rep[N];</span>
<span id="cb27-15"><a href="section-8.-model-checking-cross-validation.html#cb27-15" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span>:N) {</span>
<span id="cb27-16"><a href="section-8.-model-checking-cross-validation.html#cb27-16" aria-hidden="true" tabindex="-1"></a>    y_rep[n] = poisson_rng(lambda);</span>
<span id="cb27-17"><a href="section-8.-model-checking-cross-validation.html#cb27-17" aria-hidden="true" tabindex="-1"></a>    log_lik[n] = poisson_lpmf(y[n] | lambda);</span>
<span id="cb27-18"><a href="section-8.-model-checking-cross-validation.html#cb27-18" aria-hidden="true" tabindex="-1"></a>  }</span>
<span id="cb27-19"><a href="section-8.-model-checking-cross-validation.html#cb27-19" aria-hidden="true" tabindex="-1"></a>}</span></code></pre></div>
<hr />
</div>
</div>
<div id="chapter-7-reading-instructions" class="section level3" number="8.2.4">
<h3><span class="header-section-number">8.2.4</span> Chapter 7 reading instructions</h3>
<ul>
<li>reading
<ul>
<li>7.1 Measures of predictive accuracy</li>
<li>7.2 Information criteria and cross-validation
<ul>
<li>replace with: <em>Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC</em> <span class="citation">(<a href="#ref-Vehtari2017-st" role="doc-biblioref">Vehtari, Gelman, and Gabry 2017b</a>)</span></li>
</ul></li>
<li>7.3 Model comparison based on predictive performance
<ul>
<li>replace with: <em>Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC</em> <span class="citation">(<a href="#ref-Vehtari2017-st" role="doc-biblioref">Vehtari, Gelman, and Gabry 2017b</a>)</span></li>
</ul></li>
<li>7.4 Model comparison using Bayes factors</li>
<li>7.5 Continuous model expansion / sensitivity analysis</li>
<li>7.6 Example (may be skipped)</li>
</ul></li>
<li>extra material
<ul>
<li><a href="https://mc-stan.org/loo/reference/loo-glossary.html">LOO package glossary</a> summarizes many important terms used in the assignments <span class="citation">(<a href="#ref-R-loo" role="doc-biblioref">Vehtari et al. 2020</a>)</span> <span class="citation">(<a href="#ref-loo2017a" role="doc-biblioref">Vehtari, Gelman, and Gabry 2017a</a>)</span> <span class="citation">(<a href="#ref-loo2017b" role="doc-biblioref">Yao et al. 2017</a>)</span></li>
<li>article discussing “How should I evaluate my modelling choices?”: <em>Between the devil and the deep blue sea: Tensions between scientific judgement and statistical model selection.</em> <span class="citation">(<a href="#ref-Navarro2019-pi" role="doc-biblioref">Navarro 2019</a>)</span></li>
<li>website about model selection by Vehtari: <a href="https://avehtari.github.io/modelselection/">Model assesment, selection and inference after selection</a></li>
<li>sections 1 &amp; 5 of <em>Uncertainty in Bayesian Leave-One-Out Cross-Validation Based Model Comparison</em> <span class="citation">(<a href="#ref-Sivula2020-yw" role="doc-biblioref">Sivula, Magnusson, and Vehtari 2020</a>)</span> clarify how to interpret standard error in model comparison</li>
<li><em>A survey of Bayesian predictive methods for model assessment, selection and comparison.</em> <span class="citation">(<a href="#ref-Vehtari2012-wn" role="doc-biblioref">Vehtari and Ojanen 2012</a>)</span></li>
<li><em>Comparison of Bayesian predictive methods for model selection.</em> <span class="citation">(<a href="#ref-Piironen2017-sa" role="doc-biblioref">Piironen and Vehtari 2017</a>)</span></li>
</ul></li>
</ul>
</div>
<div id="chapter-7.-evaluating-comparing-and-expanding-models" class="section level3" number="8.2.5">
<h3><span class="header-section-number">8.2.5</span> Chapter 7. Evaluating, comparing, and expanding models</h3>
<ul>
<li>goal of this chapter is not to check model fit but to compare models and explore directions for improvement</li>
</ul>
<div id="measures-of-predictive-accuracy" class="section level4 unnumbered">
<h4>7.1 Measures of predictive accuracy</h4>
<ul>
<li>can use predictive accuracy for comparing models</li>
<li><strong>log predictive density</strong>: the logarithmic score for predictions is the log predictive density <span class="math inline">\(\log p(y|\theta)\)</span>
<ul>
<li>expected log predictive density as a measure of overall model fit</li>
</ul></li>
<li><em>external validation</em>: ideally, would check a model’s fit on out-of-sample (new) data</li>
</ul>
<div id="averaging-over-the-distirbution-of-future-data" class="section level5 unnumbered">
<h5>Averaging over the distirbution of future data</h5>
<ul>
<li><em>expected log predictive density</em> (<em>elpd</em>) for a new data point:
<ul>
<li>where <span class="math inline">\(f\)</span> is the true data-generating process and <span class="math inline">\(p_\text{post}(y)\)</span> is the posterior probability of <span class="math inline">\(y\)</span></li>
<li><span class="math inline">\(f\)</span> is usually unknown</li>
</ul></li>
</ul>
<p><span class="math display">\[
\text{elpd} =
  \text{E}_f(\log p_\text{post}(\tilde{y}_i)) =
  \int (\log p_\text{post}(\tilde{y}_i)) f(\tilde{y}_i) d\tilde{y}
\]</span></p>
<ul>
<li>for a new <em>dataset</em> (instead of a single point) of <span class="math inline">\(n\)</span> data points
<ul>
<li>kept as pointwise so can be related to cross-validation</li>
</ul></li>
</ul>
<p><span class="math display">\[
\text{elppd} =
  \text{expected log pointwise predictive density} =
  \sum_{i=1}^{n} \text{E}_f (\log p_\text{post}(\tilde{y}_i))
\]</span></p>
</div>
<div id="evaluating-predictive-accuracy-for-a-fitted-model" class="section level5 unnumbered">
<h5>Evaluating predictive accuracy for a fitted model</h5>
<ul>
<li>in practice, we do not know <span class="math inline">\(\theta\)</span> so cannot know the log predictive density <span class="math inline">\(\log p(y|\theta)\)</span></li>
<li>want to use the posterior distribution <span class="math inline">\(p_\text{post}(\theta) = p(\theta|y)\)</span> and summarize the predictive accuracy of a fitted model to data:</li>
</ul>
<p><span class="math display">\[
\begin{aligned}
\text{lppd} &amp;= \text{log pointwise predictive density} \\
  &amp;= \log \prod_{i=1}^{n} p_\text{post}(y_i) \\
  &amp;= \sum_{i=1}^{n} \log \int p(y_i|\theta) p_\text{post}(\theta) d\theta
\end{aligned}
\]</span></p>
<ul>
<li>to actually compute <em>lppd</em>, evaluate the expectation using the draws from <span class="math inline">\(p_\text{post}(\theta)\)</span>, <span class="math inline">\(\theta^s\)</span>:
<ul>
<li>this equation is a biased version of <em>elppd</em> so need to correct it (next section in information criteria)</li>
</ul></li>
</ul>
<p><span class="math display">\[
\begin{aligned}
\text{computed lppd} &amp;= \text{computed log pointwise predictive density} \\
  &amp;= \sum_{i=1}^{n} \log \lgroup \frac{1}{S} \sum_{s=1}^{S} p(y_i|\theta^s) \rgroup
\end{aligned}
\]</span></p>
</div>
</div>
<div id="information-criteria-and-cross-validation" class="section level4 unnumbered">
<h4>7.2 Information criteria and cross-validation</h4>
<ul>
<li>historically, measures of predictive accuracy are referred to as <em>information criteria</em>
<ul>
<li>are typically defined based on the deviance</li>
</ul></li>
</ul>
<div id="estimating-out-of-sample-predictive-accuracy-using-available-data" class="section level5 unnumbered">
<h5>Estimating out-of-sample predictive accuracy using available data</h5>
<ul>
<li>estimate the expected predictive accuracy without waiting for new data</li>
<li>some reasonable approximations for out-of-sample predictive accuracy
<ul>
<li><em>within-sample predictive accuracy</em>: “naive estimate of the expected log predictive density for new data is the log predictive density for existing data” (pg 170)</li>
<li><em>adjusted within-sample predictive accuracy</em>: information criteria such as WAIC</li>
<li><em>cross-validation</em>: estimate out-of-sample prediction error by fitting to training data and evaluating predictive accuracy on the held-out data</li>
</ul></li>
<li>descriptions of Akaike information criterion (AIC), deviance information criterion (DIC), and Watanabe-Akaike information criterion (or widely applicable information criterion; WAIC)
<ul>
<li>DIC and WAIC try to adjust for the <em>effective</em> number of parameters</li>
<li>WAIC is the best for Bayesian because it uses the full posterior distributions, not point estimates
<ul>
<li>there are actually two formulations for WAIC, and Gelman <em>et al.</em> recommend the second form they describe</li>
</ul></li>
</ul></li>
</ul>
</div>
<div id="effective-number-of-parameters-as-a-random-variable" class="section level5 unnumbered">
<h5>Effective number of parameters as a random variable</h5>
<ul>
<li>DIC and WAIC adjust the effective number of parameters according to the model structure and the data
<ul>
<li>the latter seems unintuitive</li>
</ul></li>
<li>example: image a simple model: <span class="math inline">\(y_i, \dots, y_n \sim N(\theta, 1)\)</span> with <span class="math inline">\(n\)</span> large and <span class="math inline">\(\theta \sim U(0, \infty)\)</span>
<ul>
<li><span class="math inline">\(\theta\)</span> is constrained to be positive but is otherwise uninformed</li>
<li>if the measure of <span class="math inline">\(y\)</span> is near 0, then the effective number of parameters <span class="math inline">\(p\)</span> is effectively <span class="math inline">\(\frac{1}{2}\)</span> because the prior removed all negative values</li>
<li>if the measure of <span class="math inline">\(y\)</span> is a large positive value, then the constraint by the prior is unimportant and the effective number of parameters <span class="math inline">\(p\)</span> is essentially 1</li>
</ul></li>
<li>Bayesian information criterion (BIC) is not comparable to AIC, DIC, and WAIC as it serves a different purpose
<ul>
<li>more discussion on pg. 175</li>
</ul></li>
</ul>
</div>
</div>
<div id="model-comparison-based-on-predictive-performance" class="section level4 unnumbered">
<h4>7.3 Model comparison based on predictive performance</h4>
<ul>
<li>in comparing “nested” models, the large model is often better fit, but is more difficult to understand and compute
<ul>
<li>“nested” models are where one contains the structure of the other and a little more
<ul>
<li>either broader priors or additional parameters</li>
</ul></li>
<li>key questions of model comparison:
<ol style="list-style-type: decimal">
<li>Is the improvement in fit large enough to justify the additional difficulty of fitting?</li>
<li>Is the prior distribution on the additional parameters reasonable?</li>
</ol></li>
</ul></li>
<li>for non-nested models, not typically interested in <em>choosing</em> one over the other, but more interested in seeing the differences between them
<ul>
<li>ideally, could construct a single model containing both of the non-nested models</li>
</ul></li>
<li>authors recommend using LOO-=CV where possible and WAIC, otherwise (pg. 182)</li>
</ul>
</div>
<div id="model-comparison-using-bayes-factor" class="section level4 unnumbered">
<h4>7.4 Model comparison using Bayes factor</h4>
<ul>
<li>generally not recommended (pg. 182 for reasons why)</li>
</ul>
</div>
<div id="continuous-model-expansion" class="section level4 unnumbered">
<h4>7.5 Continuous model expansion</h4>
<ul>
<li>posterior distributions of model parameters can either overestimate or underestimate different aspects of the “true” posterior uncertainty
<ul>
<li>overestimate uncertainty because the model usually does not contain <em>all</em> of one’s substantive knowledge</li>
<li>underestimate uncertainty by
<ul>
<li>the model is almost always wrong (i.e. imperfect) - the reason for posterior predictive checking</li>
<li>other reasonable models could have fit the data equally well - the reason for sensitivity analysis</li>
</ul></li>
</ul></li>
</ul>
<div id="adding-parameters-to-a-model" class="section level5 unnumbered">
<h5>Adding parameters to a model</h5>
<ul>
<li>reasons to expand a model:
<ol style="list-style-type: decimal">
<li>add knew parameters if the model does not fit the data or prior knowledge in some important way</li>
<li>the class of models can be broadened if some modeling assumption was unfounded</li>
<li>if two different models are under consideration, they can be combined into a larger model with a continuous parameterization that includes both models as special cases</li>
</ol>
<ul>
<li>e.g. complete-pooling and no-pooling can be combined into a hierarchical model</li>
</ul>
<ol start="4" style="list-style-type: decimal">
<li>expanding a model to include new data</li>
</ol></li>
</ul>
</div>
</div>
<div id="practical-advice-for-model-checking-and-expansion" class="section level4 unnumbered">
<h4>Practical advice for model checking and expansion</h4>
<ul>
<li>examine posterior distributions of substantively important parameters and predicted quantities
<ul>
<li>e.g. number of zeros in a count model</li>
<li>maximum and minimum predicted values</li>
</ul></li>
<li>compare posterior distributions and posterior predictions with substantive knowledge
<ul>
<li>this includes the observed data</li>
</ul></li>
</ul>
</div>
</div>
<div id="additional-reading" class="section level3" number="8.2.6">
<h3><span class="header-section-number">8.2.6</span> Additional Reading</h3>
<div id="visualization-in-bayesian-workflow" class="section level4 unnumbered">
<h4><em>Visualization in Bayesian workflow</em></h4>
<p>(<a href="https://github.com/jhrcook/bayesian-data-analysis-course/tree/master/reading/Visualization-in-Bayesian-workflow.pdf">pdf</a>, <a href="https://doi.org/10.1111/rssa.12378">link</a>)</p>
<ul>
<li>the phases of statistical workflow:
<ol style="list-style-type: lower-alpha">
<li>exploratory data analysis</li>
</ol>
<ul>
<li>aid in setting up an initial model</li>
</ul>
<ol start="2" style="list-style-type: lower-alpha">
<li>computational model checks using fake-data simulation and the prior predictive distribution</li>
<li>computational checks to ensure the inference algorithm works reliably</li>
<li>posterior predictive checks and other juxtapositions of data and predictions under the fitted model</li>
<li>model comparison via tools such as cross-validation</li>
</ol></li>
</ul>
<div id="fake-data-can-be-almost-as-valuable-as-real-data-for-building-your-model" class="section level5 unnumbered">
<h5>3. Fake data can be almost as valuable as real data for building your model</h5>
<ul>
<li>visualize simulations from the prior marginal distribution of the data to assess the consistency of the chosen priors with domain knowledge</li>
<li><em>weakly informative prior</em>: if draws from the prior data generating distribution <span class="math inline">\(p(y)\)</span> could represent any dataset that could plausibly be observed
<ul>
<li>this prior predictive distribution for the data has at least some mass around extreme but plausible data sets</li>
<li>there should be no mass on completely implausible data sets</li>
<li>generate a “flip book” of simulated datasets that can be used to investigate the variability and multivariate structure of the distribution</li>
</ul></li>
</ul>
</div>
<div id="graphical-markov-chain-monte-carlo-diagnostics-moving-beyond-trace-plots" class="section level5 unnumbered">
<h5>4. Graphical Markov chain Monte Carlo diagnostics: moving beyond trace plots</h5>
<ul>
<li>catching divergent draws heuristically is a powerful feature of HMC
<ul>
<li>sometimes get falsely flagged, so must check that the divergences were infact outside of the typical set</li>
<li>two additional plots for diagnosing troublesome areas of the parameter space
<ol style="list-style-type: decimal">
<li>bivariate scatterplots that highlight divergent transitions</li>
</ol>
<ul>
<li>bad sign: if the divergences were clustered</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>parallel coordinate plot</li>
</ol>
<ul>
<li>bad sign: the divergences would have similar structure</li>
</ul></li>
</ul></li>
</ul>
<div class="figure">
<img src="notes-assets/08_model-checking-and-cv_bda3-6-7/gabry_2007_fig5.png" alt="" />
<p class="caption">mcmc-diagnositcs-fig</p>
</div>
</div>
</div>
<div id="how-did-we-do-posterior-predictive-checks-are-vital-for-model-evaluation" class="section level4 unnumbered">
<h4>5. How did we do? Posterior predictive checks are vital for model evaluation</h4>
<ul>
<li>“The idea behind posterior predictive checking is simple: if a model is a good fit we should be able to use it to generate data that resemble the data we observed”
<ul>
<li>“can also perform similar checks within levels of a grouping variable”</li>
</ul></li>
<li>check that predictions are calibrated using LOO-CV predictive cumulative density function values which should be uniform (for continuous data)</li>
</ul>
</div>
<div id="pointwise-plots-for-predictive-model-comparison" class="section level4 unnumbered">
<h4>6. Pointwise plots for predictive model comparison</h4>
<ul>
<li>identify unusual points in the data
<ul>
<li>are other either outliers or points with high leverage</li>
<li>indicate how the model can be modified to better fut the data</li>
</ul></li>
<li>main tool for this analysis is the LOO predictive distribution <span class="math inline">\(p(y_i|y_{-i})\)</span>
<ul>
<li>examine LOO log-predictive density values to find observations that are difficult to predict</li>
<li>can be used for model comparison by checking which model best captures each held-out data point</li>
</ul></li>
<li>also compare the full data log-posterior predictive density against each LOO log-predictive density to see which data points are difficult to model but not very influential
<ul>
<li>this is automatically calculated in PSIS-LOO as the parameter <span class="math inline">\(\hat{k}\)</span></li>
</ul></li>
</ul>
</div>
<div id="practical-bayesian-model-evaluation-using-leave-one-out-cross-validation-and-waic" class="section level4 unnumbered">
<h4><em>Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC</em></h4>
<p>(<a href="https://github.com/jhrcook/bayesian-data-analysis-course/tree/master/reading/Practical-Bayesian-model-evaluation-using-leave-one-out-cross-validation-and-WAIC.pdf">pdf</a>, <a href="https://arxiv.org/abs/1507.04544">link</a>)</p>
<blockquote>
<p>Here we lay out fast and stable computations for LOO and WAIC that can be performed usingexisting simulation draws.
We introduce an efficient computation of LOO using Pareto-smoothed importance sampling (PSIS), a new procedure for regularizing importance weights.
Although WAIC is asymptotically equal to LOO, we demonstrate that PSIS-LOO is more robust in thefinite case with weak priors or influential observations.</p>
</blockquote>
<div id="introduction-1" class="section level5 unnumbered">
<h5>Introduction</h5>
<ul>
<li>exact CV requires re-fitting the model with every new training set</li>
<li>can approximate this with LOO-CV using importance sampling, but results can be very noisy</li>
<li>use <em>Pareto smoothed importance sampling</em> (PSIS) to calculate a more accurate and stable estimate
<ul>
<li>fit a Pareto distribution to the upper tail of the distribution of the importance weights</li>
</ul></li>
<li>this paper demonstrates that PSIS-LOO is better than WAIC in the finite case
<ul>
<li>also provide diagnostics for which method, WAIC or PSIS-LOO, is better or whether <em>k</em>-fold CV should be used instead</li>
</ul></li>
</ul>
</div>
<div id="estimating-out-of-sample-pointwise-predictive-accuracy-using-posterior-simulations" class="section level5 unnumbered">
<h5>Estimating out-of-sample pointwise predictive accuracy using posterior simulations</h5>
<ul>
<li>posterior predictive distribution: <span class="math inline">\(p(\tilde{y}|y) = \int p(\tilde{y}_i|\theta) p(\theta|y) d\theta\)</span></li>
<li>expected log pointwise predictive density (ELPD)
<ul>
<li>measure of predictive accuracy for the <span class="math inline">\(n\)</span> data points in a dataset</li>
<li><span class="math inline">\(p_t(\tilde{y}_i)\)</span>: distribution of the true data-generating process for <span class="math inline">\(\tilde{y}_i\)</span></li>
<li><span class="math inline">\(p_t(\tilde{y}_i)\)</span> is usually unknown so CV and WAIC are used to approximate ELPD</li>
</ul></li>
</ul>
<p><span class="math display">\[
\text{elpd} = \sum_{i=1}^n \int p_t(\tilde{y}_i) \log p(\tilde{y}_i|y) d\tilde{y}_i
\]</span></p>
<ul>
<li>useful quantity is the log pointwise predictive density (LPD)
<ul>
<li>“The LPD of observed data <span class="math inline">\(y\)</span> is an overestimate of the ELPD for future data.”</li>
</ul></li>
</ul>
<p><span class="math display">\[
\text{lpd} = \sum_{i=1}^n \log p(y_i|y) = \sum_{i=1}^n \int p(y_i|\theta) p(\theta|y) d\theta
\]</span></p>
<ul>
<li>to compute LPD in practice, evaluate the expectation using draws for the posteior <span class="math inline">\(p_\text{post}(\theta)\)</span>, <span class="math inline">\(\theta^s\)</span> for <span class="math inline">\(s = 1, \dots, S\)</span>
<ul>
<li><span class="math inline">\(\widehat{lpd}\)</span>: computed log pointwise predictive density</li>
</ul></li>
</ul>
<p><span class="math display">\[
\widehat{lpd} = \sum_{i=1}^n \lgroup \frac{1}{S} \sum_{s=1}^S p(y_i|\theta^s) \rgroup
\]</span>
##### Pareto smoothed importance sampling {-}</p>
<ul>
<li>fit the right tail of importance weights to smooth the values</li>
<li>The estimated shape parameter <span class="math inline">\(\hat{k}\)</span> of the generalized Pareto distribution can be used to assessthe reliability of the estimate:
<ul>
<li><span class="math inline">\(k &lt; \frac{1}{2}\)</span>: the variance of the raw importance ratios is finite, the central limit theorem holds, and the estimate converges quickly</li>
<li><span class="math inline">\(\frac{1}{2} &lt; k &lt; 1\)</span>: the variance of the raw importance ratios is infinite but the mean exists, the generalized central limit theorem for stable distributions holds, and the convergence of the estimate is slower
<ul>
<li>the variance of the PSIS estimate is finite but may be large</li>
</ul></li>
<li><span class="math inline">\(k &gt; 1\)</span>: the variance and the mean of the raw ratios distribution do not exist
<ul>
<li>the variance of the PSIS estimate is finite but may be large</li>
</ul></li>
</ul></li>
<li>“If the estimated tail shape parameter <span class="math inline">\(\hat{k}\)</span> exceeds 0.5, the user should be warned, although in practice we have observed good performance for values of<span class="math inline">\(\hat{k}\)</span> up to 0.7.”</li>
<li>if the PSIS estimate has a finite variance, when <span class="math inline">\(\hat{k} &gt; 0.7\)</span> the user should consider:
<ol style="list-style-type: decimal">
<li>sampling directly from <span class="math inline">\(p(\theta^s|y_{−i})\)</span> for the problematic <span class="math inline">\(i\)</span>,</li>
<li>use <em>k</em>-fold CV, or</li>
<li>use a more robust model</li>
</ol></li>
</ul>
</div>
</div>
<div id="model-assesment-selection-and-inference-after-selection" class="section level4 unnumbered">
<h4>Model assesment, selection and inference after selection</h4>
<p>(<a href="https://avehtari.github.io/modelselection/">link</a>)</p>
</div>
<div id="cross-validation-faq" class="section level4 unnumbered">
<h4>Cross-validation FAQ</h4>
<p>(<a href="https://avehtari.github.io/modelselection/CV-FAQ.html">link</a>)</p>
</div>
</div>
<div id="chapter-7.-lecture-notes" class="section level3" number="8.2.7">
<h3><span class="header-section-number">8.2.7</span> Chapter 7. Lecture notes</h3>
<div id="lecture-8.2.-cross-validation-part-1" class="section level4 unnumbered">
<h4>Lecture 8.2. Cross-Validation (part 1)</h4>
<ul>
<li>predictive performance
<ul>
<li>true predictive performance can be found by making predictions on new data and comparing to true observation
<ul>
<li><em>external validation</em></li>
</ul></li>
<li><em>expected</em> predictive performance as an approximation</li>
</ul></li>
<li>calculating the posterior predictive density for a data point:
<ul>
<li>generate a posterior predictive distribution for the data point</li>
<li>find the density of the distribution at the actual observed value</li>
</ul></li>
<li>good walk-through of calculating posterior predictive distributions and LOO analyses</li>
<li>some discussion of making predictions in parts of <span class="math inline">\(x\)</span> not in the data set
<ul>
<li>e.g. predictions in the future of time series models</li>
</ul></li>
</ul>

</div>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Navarro2019-pi" class="csl-entry">
Navarro, Danielle J. 2019. <span>“Between the Devil and the Deep Blue Sea: Tensions Between Scientific Judgement and Statistical Model Selection.”</span> <em>Computational Brain &amp; Behavior</em> 2 (1): 28–34. <a href="https://doi.org/10.1007/s42113-018-0019-z">https://doi.org/10.1007/s42113-018-0019-z</a>.
</div>
<div id="ref-Piironen2017-sa" class="csl-entry">
Piironen, Juho, and Aki Vehtari. 2017. <span>“Comparison of Bayesian Predictive Methods for Model Selection.”</span> <em>Stat. Comput.</em> 27 (3): 711–35. <a href="https://doi.org/10.1007/s11222-016-9649-y">https://doi.org/10.1007/s11222-016-9649-y</a>.
</div>
<div id="ref-Sivula2020-yw" class="csl-entry">
Sivula, Tuomas, Måns Magnusson, and Aki Vehtari. 2020. <span>“Uncertainty in Bayesian <span>Leave-One-Out</span> <span>Cross-Validation</span> Based Model Comparison,”</span> August. <a href="https://arxiv.org/abs/2008.10296">https://arxiv.org/abs/2008.10296</a>.
</div>
<div id="ref-R-loo" class="csl-entry">
Vehtari, Aki, Jonah Gabry, Mans Magnusson, Yuling Yao, Paul-Christian Bürkner, Topi Paananen, and Andrew Gelman. 2020. <em>Loo: Efficient Leave-One-Out Cross-Validation and WAIC for Bayesian Models</em>. <a href="https://CRAN.R-project.org/package=loo">https://CRAN.R-project.org/package=loo</a>.
</div>
<div id="ref-loo2017a" class="csl-entry">
Vehtari, Aki, Andrew Gelman, and Jonah Gabry. 2017a. <span>“Practical Bayesian Model Evaluation Using Leave-One-Out Cross-Validation and WAIC.”</span> <em>Statistics and Computing</em> 27: 1413–32. <a href="https://doi.org/10.1007/s11222-016-9696-4">https://doi.org/10.1007/s11222-016-9696-4</a>.
</div>
<div id="ref-Vehtari2017-st" class="csl-entry">
———. 2017b. <span>“Practical Bayesian Model Evaluation Using Leave-One-Out Cross-Validation and <span>WAIC</span>.”</span> <em>Stat. Comput.</em> 27 (5): 1413–32. <a href="https://doi.org/10.1007/s11222-016-9696-4">https://doi.org/10.1007/s11222-016-9696-4</a>.
</div>
<div id="ref-Vehtari2012-wn" class="csl-entry">
Vehtari, Aki, and Janne Ojanen. 2012. <span>“A Survey of Bayesian Predictive Methods for Model Assessment, Selection and Comparison.”</span> <em>Ssu</em> 6 (none): 142–228. <a href="https://doi.org/10.1214/12-SS102">https://doi.org/10.1214/12-SS102</a>.
</div>
<div id="ref-loo2017b" class="csl-entry">
Yao, Yuling, Aki Vehtari, Daniel Simpson, and Andrew Gelman. 2017. <span>“Using Stacking to Average Bayesian Predictive Distributions.”</span> <em>Bayesian Analysis</em>. <a href="https://doi.org/10.1214/17-BA1091">https://doi.org/10.1214/17-BA1091</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="section-7.-hierarchical-models-and-exchangeability.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="section-9.-model-comparison-and-selection.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/hadley/r4ds/edit/master/notes-08_model-checking-and-cv_bda3-6-7.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/jhrcook/bayesian-data-analysis-course/blob/master/notes-08_model-checking-and-cv_bda3-6-7.Rmd",
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
