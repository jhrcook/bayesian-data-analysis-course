[["index.html", "1 Bayesian Data Analysis course 1.1 Resources 1.2 How to study 1.3 Course sections 1.4 Additional notes 1.5 Stan models", " 1 Bayesian Data Analysis course TODO: update this page to act as more of an introduction and explain the structure of the book go through each Rmd and mark “unnumbered” subsections footnotes: remove number from foot note so they appear at the bottom of the current page (https://bookdown.org/yihui/bookdown/markdown-syntax.html#inline-formatting) citations: https://bookdown.org/yihui/bookdown/citations.html#citations figure out where to put Stan models 1.1 Resources Course website 2021 Schedule GitHub repo (my fork) Bayesian Data Analysis (3e) (BDA3) (exercise solutions) Chapter Notes Video lectures or individually lists here Lecture slides 1.2 How to study The following are recommendations from the course creators on how to take the course. The recommended way to go through the material is: Read the reading instructions for a chapter in the chapter notes. Read the chapter in BDA3 and check that you find the terms listed in the reading instructions. Watch the corresponding video lecture to get explanations for most important parts. Read corresponding additional information in the chapter notes. Run the corresponding demos in R demos or Python demos. Read the exercise instructions and make the corresponding assignments. Demo codes in R demos and Python demos have a lot of useful examples for handling data and plotting figures. If you have problems, visit TA sessions or ask in course slack channel. If you want to learn more, make also self study exercises listed below. 1.3 Course sections Section Notes Book exercises Assignments 1. Course Introduction notes exercises assignment 1 2. Basics of Bayesian Inference notes exercises assignment 2 3. Multidimensional Posterior notes (none) assignment 3 4. Monte Carlo notes (none) assignment 4 5. Markov chain Monte Carlo notes (none) assignment 5 6. HMC, NUTS, and Stan notes (none) assignment 6 7. Hierarchical models and exchangeability notes exercises assignment 7 8. Model checking &amp; Cross-validation notes (none) (none) 9. Model comparison and selection notes (none) assignment 8 10. Decision analysis notes (none) assignment 9 11. Normal approximation &amp; Frequency properties notes (none) (none) 12. Extended topics notes (none) (none) 1.4 Additional notes Chapter Title Notes Additional work 14 Introduction to regression models notes 15 Hierarchical linear models notes 16 Generalized linear models 18 Models for missing data 19 Parametric nonlinear models notes serial dilution model 20 Basis function models notes 21 Gaussian process models notes 22 Finite mixture models notes 23 Dirichlet process models notes 1.5 Stan models Drug bioassay model for Assignment 6 8 school SAT model Drownings for Assignment 7 Factory machine measurements for Assignments 7 &amp; 8: pooled separate hierarchical (also used in Assignment 9) serial dilution assay for chapter 19 exercises "],["introduction.html", "Introduction", " Introduction TODO: add more information here; describe the general structure of the notes. These are notes from the lectures and books for each section of the course. "],["section-1.-course-introduction-and-prerequisites.html", "2 Section 1. Course introduction and prerequisites 2.1 Resources 2.2 Notes", " 2 Section 1. Course introduction and prerequisites 2021-08-17 2.1 Resources BDA chapter 1 and reading instructions lectures: ‘Computational probabilistic modeling’ ‘Introduction to uncertainty and modelling’ video: ‘Introduction to the course contents’ slides Assignment 1 2.2 Notes 2.2.1 Reading instructions model vs. likelihood for \\(p(y|\\theta, M)\\) model when the function is in terms of \\(y\\) should be written as \\(p_y(y|\\theta, M)\\) used to describe uncertainty about \\(y\\) given values of \\(\\theta\\) and \\(M\\) likelihood when in the function is in terms of \\(\\theta\\) should be written as \\(p_\\theta(y|\\theta, M)\\) the posterior distribution describes the probability for different values of \\(\\theta\\) given fixed values for \\(y\\) “The likelihood function is unnormalized probability distribution describing uncertainty related to \\(\\theta\\) (and that’s why Bayes rule has the normalization term to get the posterior distribution).” exchangeability independence is stronger condition than exchangeability independence implies exchangeability exchangeability does not imply independence exchangeability is related to what information is available instead of the properties of unknown underlying data generating mechanism 2.2.2 Lecture notes 2.2.2.1 Introduction to uncertainty and modelling two types of uncertainty: aleatoric: due to randomness epistemic: due to lack of knowledge model vs. likelihood: model \\(p_y(y|\\theta, M)\\) a function of \\(y\\) given fixed values of \\(\\theta\\) describes aleatoric uncertainty likelihood \\(p_\\theta(y|\\theta, M)\\) function of \\(\\theta\\) given fixed values of \\(y\\) provides information about the epistemic uncertainty is not a probability distribution Bayes rule combines the likelihood with prior uncertainty to update the posterior uncertainty example with a bag containing red and yellow chips: probability of red = #red / #red + #yellow = \\(\\theta\\) \\(p(y = \\text{red} | \\theta)\\): aleatoric uncertainty predicting the probability of pulling a red chip has uncertainty due to randomness even if we new \\(\\theta\\) exactly \\(p(\\theta)\\): epistemic uncertainty we don’t know \\(\\theta\\) but could compute it exactly if we knew the contents of the bag 2.2.2.2 Introduction to the course contents benefits of Bayesian approach integrate over uncertainties to focus on interesting parts use relevant prior information hierarchical models model checking and evaluation "],["section-2.-basics-of-bayesian-inferences.html", "3 Section 2. Basics of Bayesian inferences 3.1 Resources 3.2 Notes", " 3 Section 2. Basics of Bayesian inferences 2021-08-21 3.1 Resources BDA3 chapter 2 and reading instructions lectures: ‘2.1 Basics of Bayesian inference, observation model, likelihood, posterior and binomial model, predictive distribution and benefit of integration’ ‘2.2 Priors and prior information, and one parameter normal model’ ‘Extra explanations about likelihood, normalization term, density, and conditioning on model M’ (optional) ‘Summary 2.1. Observation model, likelihood, posterior and binomial model’ (optional) ‘Summary 2.2. Predictive distribution and benefit of integration’ (optional) ‘Summary 2.3. Priors and prior information’ (optional) slides and extra slides Assignment 2 3.2 Notes 3.2.1 Chapter instructions recommendations about weakly informative priors has changed a bit updated recommendations: Prior Choice Recommendations “5 levels of priors”: Flat prior (not usually recommended) Super-vague but proper prior: \\(N(0, 10^6)\\) (not usually recommended) Weakly informative prior: very weak; \\(N(0, 10)\\) Generic weakly informative prior: \\(N(0, 1)\\) Specific informative prior: \\(N(0.4, 0.2)\\) or whatever; can sometimes be expressed as a scaling followed by a generic prior: \\(\\theta = 0.4 + 0.2z; \\text{ } z \\sim N(0, 1)\\) “flat and super-vague priors are not usually recommended” even a seemingly weakly informative prior could be informative e.g. a prior of \\(N(0, 1)\\) could put weight on values too large if a large effect size would only be on the scale of 0.1 def. weakly informative: “if there’s a reasonably large amount of data, the likelihood will dominate, and the prior will not be important” section on General Principles; some stand-outs copied here: “Computational goal in Stan: reducing instability which can typically arise from bad geometry in the posterior” “Weakly informative prior should contain enough information to regularize: the idea is that the prior rules out unreasonable parameter values but is not so strong as to rule out values that might make sense” “When using informative priors, be explicit about every choice; write a sentence about each parameter in the model.” 3.2.2 Chapter 2. Single-parameter models I took notes in the book, so below are just some main points. 2.2 Posterior as compromise between data and prior information “general feature of Bayesian inference: the posterior distribution is centered at a point that represents a compromise between the prior information and the data” 2.3 Estimating a probability from binomial data a key benefit of Bayesian modeling is the flexibility of summarizing posterior probabilities can be used to answer the key research questions commonly used summary statistics centrality: mean, median, mode variation: standard deviation, interquartile range, highest posterior density 2.4 Informative prior distributions hyperparameter: parameter of a prior distribution conjugacy: “the property that the posterior distribution follows the same parameter form as the prior distribution” e.g. the beta prior is a conjugate family for the binomial likelihood e.g. the gamma prior is a conjugate family for the Poisson likelihood convenient because the posterior follows a known parametric family formal definition of conjugacy: \\[ p(\\theta | y) \\in \\mathcal{P} \\text{ for all } p(\\cdot | \\theta) \\in \\mathcal{F} \\text{ and } p(\\cdot) \\in \\mathcal{P} \\] 2.5 Normal distribution with known variance precision (when discussing normal distributions): the inverse of the variance \\(\\frac{1}{\\tau^2}\\) 2.6 Other standard single-parameter models Poisson model for count data data \\(y\\) is the number of positive events unknown rate of the events \\(\\theta\\) conjugate prior is the gamma distribution section 2.7 is a good example of a hierarchical Poisson model 2.8 Noninformative prior distributions See more information in the notes from the chapter instructions. rationale: let the data speak for themselves; inferences are unaffected by external information/bias problems: can cause the posterior to become improper computationally, makes it harder to sample from the posterior 2.9 Weakly informative prior distributions See more information in the notes from the chapter instructions. weakly informative: the prior is proper, but intentionally weaker than whatever actual prior knowledge is available “in general, any problem has some natural constraints that would allow a weakly informative model” small amount of real-world knowledge to ensure the posterior makes sense 3.2.3 Lecture notes 2.1 Basics of Bayesian inference, observation model, likelihood, posterior and binomial model, predictive distribution and benefit of integration predictive distribution “integrate over uncertainties” for the example of pulling red or yellow chips out of a bag: want a predictive distribution for some data point \\(\\tilde{y} = 1\\): if we know \\(\\theta\\) then it is easy: \\(p(\\tilde{y} = 1 | \\theta, y, n, M)\\) where \\(n\\) is number of draws, \\(y\\) is number of success (red chip), \\(M\\) is model we don’t know \\(\\theta\\), we weight the probability of the new data for a given \\(\\theta\\) by the posterior probability that \\(\\theta\\) is that value sum (integrate) over all possible values for \\(\\theta\\) (“integrate out the uncertainty of \\(\\theta\\)”) \\(p(\\tilde{y}=1|y, n, M) = \\int_0^1 p(\\tilde{y} = 1 | \\theta, y, n, M) p(\\theta | y, n, M) d\\theta\\) now the prediction is not conditioned on \\(\\theta\\), just on what was observed prior predictive: predictions before seeing any data \\(p(\\tilde{y}=1|M) = \\int_o^1 p(\\tilde{y}=1 | \\theta, y, n, M) p(\\theta|M)\\) 2.2 Priors and prior information, and one parameter normal model proper prior: \\(\\int p(\\theta) = 1\\) better to use proper priors improper prior density does not have a finite integral the posterior can sometimes still be proper, though uniform distributions to infinity are improper a weak prior is not non-informative could give a lot of a weight to very unlikely (or impossible) values make sure to check prior values against knowable values sufficient statistic: the quantity \\(t(y)\\) is a sufficient statistic for \\(\\theta\\) because the likelihood for \\(\\theta\\) depends on the data \\(y\\) only through the value of \\(t(y)\\) smaller dimensional data that fully summarizes the full data can define a Gaussian with just the mean and s.d. Extras: likelihood, normalization term, density, and conditioning on model M Predictive distribution and benefit of integration predictive dist. effect of integration predictive dist of new \\(\\hat{y}\\) (discrete) with model \\(M\\): if we know \\(\\theta\\): \\(p(\\hat{y} = 1| y, n, M) = p(\\hat{y} = 1 | \\theta, y, n, M)\\) if we don’t know \\(\\theta\\): \\(p(\\hat{y} = 1 | \\theta, y, n, M) p(\\theta| y, n, M)\\) weight by the probability of the value for \\(\\theta\\) integrate over all possible values of \\(\\theta\\): \\(p(\\hat{y} = 1|y, n, M) = \\int_0^1 p(\\hat{y}=1| \\theta, y, n, M) p(\\theta|y, n, M)d\\theta\\) “integrate out the uncertainty over \\(\\theta\\)” prior predictive for new data \\(\\hat{y}\\): \\(p(\\hat{y} =1|M) = int_0^1 p(\\hat{y}=1|\\theta,y,n,M)p(\\theta|M)\\) Priors and prior information conjugate priors do not result in any computational benefits in HMC or NUTS can be useful to analytically reduce the size of a model, beforehand, though "],["section-3.-multidimensional-posterior.html", "4 Section 3. Multidimensional Posterior 4.1 Resources 4.2 Notes", " 4 Section 3. Multidimensional Posterior 2021-09-02 4.1 Resources BDA3 chapter 3 and reading instructions lectures: ‘Lecture 3. Multiparameter models, joint, marginal and conditional distribution, normal model, bioassay example, grid sampling and grid evaluation’ slides Assignment 3 4.2 Notes 4.2.1 Reading instructions the trace of a square matrix \\(tr(A)\\) is the sum of the diagonals the following property is used in derivation of 3.11: \\(tr(ABC) = tr(CAB) = tr(BCA)\\) 4.2.2 Chapter 3. Introduction to multiparameter models 4.2.2.1 Averaging over ‘nuisance parameters’ suppose the unknown variable \\(\\theta\\) is a vector of length two: \\(\\theta= (\\theta_1, \\theta_2)\\) may only care about one of the variables, but the other is still required for a good model example model: \\(y | \\mu, \\sigma^2 \\sim N(\\mu, \\sigma^2)\\) here, \\(\\theta\\) would be the unknown values \\(\\mu (=\\theta_1)\\) and \\(\\sigma (=\\theta_2)\\), but we really only care about \\(\\mu\\) we want \\(p(\\theta_1|y)\\) derive it from the joint posterior density: \\(p(\\theta_1, \\theta_2) \\propto p(y|\\theta_1, \\theta2) p(\\theta_1, \\theta_2)\\) by averaging over \\(\\theta_2\\): \\(p(\\theta_1|y) = \\int p(\\theta_1, \\theta_2| y) d\\theta_2\\) “integrate over the uncertainty in \\(\\theta_2\\)” 4.2.2.2 Summary of elementary modeling and computation the following is an outline of a simple Bayesian analysis it will change when we get to more complex models whose posteriors are estimated by more complex sampling processes write the likelihood: \\(p(y|\\theta)\\) write the posterior density: \\(p(\\theta|y) \\propto p(\\theta) p(y|\\theta)\\)$ estimate the parameters \\(\\theta\\) (e.g. using MLE) draw simulations \\(\\theta^1, \\dots, \\theta^S\\) for the posterior distribution (using the results of 3 as a starting point); use the samples to compute any other functions of \\(\\theta\\) that are of interest if any predictive quantities \\(\\tilde{y}\\) are of interest, simulate \\(\\tilde{y}^1, \\dots, \\tilde{y}^S\\) from \\(p(\\tilde{y} | \\theta^s)\\) 4.2.3 Lecture notes (No extra notes were taken — some comments added directly to slides.) "],["introduction-1.html", "Introduction", " Introduction TODO: add more information here; describe the general structure of the notes. These are the course assignments. "],["assignment-1.html", "5 Assignment 1 5.1 Setup 5.2 Exercise 1 5.3 Exercise 3 5.4 Exercise 4 5.5 Exercise 5", " 5 Assignment 1 2021-08-19 Assignment 1 5.1 Setup library(glue) 5.2 Exercise 1 (Basic probability theory notation and terms). This can be trivial or you may need to refresh your memory on these concepts. Note that some terms may be different names for the same concept. Explain each of the following terms with one sentence: probability: how likely some assertion is to be true probability mass: how likely a discrete random variable is to be some value probability density: how likely a continuous random variable is to be some value probability mass function (pmf): a function describing how likely a discrete random variable is to be any possible value probability density function (pdf): a function describing how likely a continuous random variable is to be any possible value probability distribution: a function describing how likely a random variable is to be of some value discrete probability distribution: a probability distribution over discrete values continuous probability distribution: a probability distribution over continuous values cumulative distribution function (cdf): the cumulative sum of probabilities from a probability distribution likelihood: how probable some event is under a given hypothesis 5.3 Exercise 3 (Bayes’ theorem) A group of researchers has designed a new inexpensive and painless test for detecting lung cancer. The test is intended to be an initial screening test for the population in general. A positive result (presence of lung cancer) from the test would be followed up immediately with medication, surgery or more extensive and expensive test. The researchers know from their studies the following facts: Test gives a positive result in 98% of the time when the test subject has lung cancer. Test gives a negative result in 96 % of the time when the test subject does not have lung cancer. In general population approximately 1 person in 1000 has lung cancer. The researchers are happy with these preliminary results (about 97% success rate), and wish to get the test to market as soon as possible. How would you advise them? Base your answer on Bayes’ rule computations. Hint: Relatively high false negative (cancer doesn’t get detected) or high false positive (unnecessarily administer medication) rates are typically bad and undesirable in tests. Hint: Here are some probability values that can help you figure out if you copied the right conditional probabilities from the question. \\(P(\\text{Test gives positive} | \\text{Subject does not have lung cancer}) = 0.04\\) \\(P(\\text{Test gives positive and Subject has lung cancer}) = 0.00098\\) this is also referred to as the joint probability of test being positive and the subject having lung cancer. We are interested in the false positive and false negative rates. The false positive rate, a positive test when the patient does not have cancer, is calculated below using Bayes’ rule: \\[ \\Pr(\\text{no cancer} | \\text{positive test}) = \\frac{\\Pr(\\text{no cancer}) \\Pr(\\text{positive test} | \\text{no cancer})}{\\Pr(\\text{positive test})} \\] Each of the components: \\[ \\Pr(\\text{no cancer}) = 999/1000\\\\ \\Pr(\\text{positive test} | \\text{no cancer}) = 4\\% = 4/100\\\\ \\begin{aligned} \\Pr(\\text{positive test}) &amp;= \\Pr(\\text{positive test AND cancer}) + \\Pr(\\text{positive test AND no cancer}) \\\\ &amp;= \\frac{98}{100} \\frac{1}{1000} + \\frac{4}{100} \\frac{999}{1000} = \\frac{4094}{100000} \\end{aligned} \\] thus \\[ \\begin{aligned} \\Pr(\\text{no cancer} | \\text{positive test}) &amp;= \\frac{\\Pr(\\text{no cancer}) \\Pr(\\text{positive test} | \\text{no cancer})}{\\Pr(\\text{positive test})} \\\\ &amp;= \\frac{\\frac{999}{1000} \\frac{4}{100}}{\\frac{4094}{100000}} \\\\ &amp;= 0.976 \\\\ &amp;= 97.6 \\% \\end{aligned} \\] The false positive rate, a negative test when the patient does have cancer, is calculated below using Bayes’ rule: \\[ \\Pr(\\text{cancer} | \\text{negative test}) = \\frac{\\Pr(\\text{cancer}) \\Pr(\\text{negative test} | \\text{cancer})}{\\Pr(\\text{negative test})} \\] Each of the components: \\[ \\Pr(\\text{cancer}) = 1/1000\\\\ \\Pr(\\text{negative test} | \\text{cancer}) = 1-0.98 = 2/100 \\\\ \\begin{aligned} \\Pr(\\text{negative test}) &amp;= \\Pr(\\text{negative test AND cancer}) + \\Pr(\\text{negative test AND no cancer}) \\\\ &amp;= \\frac{2}{100} \\frac{1}{1000} + \\frac{96}{100} \\frac{999}{1000} = \\frac{95906}{100000} \\end{aligned} \\] Thus, \\[ \\begin{aligned} \\Pr(\\text{cancer} | \\text{negative test}) &amp;= \\frac{\\Pr(\\text{cancer}) \\Pr(\\text{negative test} | \\text{cancer})}{\\Pr(\\text{negative test})} \\\\ &amp;= \\frac{\\frac{1}{1000} \\frac{2}{100}}{\\frac{95906}{100000}} \\\\ &amp;= 0.0000209 \\\\ &amp;= 0.00209 \\% \\end{aligned} \\] The false negative rate is quite low, primarily because the cancer is relatively rare to begin with. But, for the same reason, the false positive rate is very high. Therefore, it could be advised that positive tests are confirmed with an independently-conducted second test or another testing procedure (e.g. CT scan). Taking the test twice will only help if the false positive is caused by randomness and is not due to some other factor of the patient (e.g. the test is picking up some metabolite this patient produces due to their specific diet). 5.4 Exercise 4 We have three boxes, A, B, and C. There are 2 red balls and 5 white balls in the box A, 4 red balls and 1 white ball in the box B, and 1 red ball and 3 white balls in the box C. Consider a random experiment in which one of the boxes is randomly selected and from that box, one ball is randomly picked up. After observing the color of the ball it is replaced in the box it came from. Suppose also that on average box A is selected 40% of the time and box B 10% of the time (i.e. P (A) = 0.4). What is the probability of picking a red ball? If a red ball was picked, from which box it most probably came from? Implement two functions in R that computes the probabilities. 5.4.1 4.a \\[ \\Pr(red) = \\Sigma_{b}^{boxes} \\Pr(\\text{red} | \\text{box}_b) \\Pr(\\text{box}_b) \\] # Contents of the boxes. boxes &lt;- matrix( c(2, 4, 1, 5, 1, 3), ncol = 2, dimnames = list(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), c(&quot;red&quot;, &quot;white&quot;)) ) # Probability of selecting each box. box_probs &lt;- matrix( c(0.4, 0.1, 0.5), ncol = 1, dimnames = list(c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;)) ) # Calculate the probability of red per box. p_red_per_box &lt;- function(boxes) { return(boxes[, &quot;red&quot;] / apply(boxes, 1, sum)) } # Calculate the probability of pulling red. p_red &lt;- function(boxes, box_probs) { red_probs &lt;- p_red_per_box(boxes) return(sum(red_probs * box_probs)) } prob_red &lt;- p_red(boxes, box_probs) print(glue(&quot;probability of red: {prob_red}&quot;)) #&gt; probability of red: 0.319285714285714 # Calculate the prob. that each box was used given a red ball was selected. p_box &lt;- function(boxes, box_probs) { prob_red &lt;- p_red(boxes, box_probs) red_probs &lt;- p_red_per_box(boxes) box_probs_red &lt;- c() for (box in rownames(box_probs)) { p &lt;- unlist(box_probs[box, ]) * unlist(red_probs[box]) / prob_red box_probs_red &lt;- c(box_probs_red, p) } names(box_probs_red) &lt;- rownames(box_probs) return(box_probs_red) } probs_of_boxes &lt;- p_box(boxes = boxes, box_probs = box_probs) stopifnot(sum(probs_of_boxes) == 1) probs_of_boxes #&gt; A B C #&gt; 0.3579418 0.2505593 0.3914989 5.5 Exercise 5 Assume that on average fraternal twins (two fertilized eggs and then could be of different sex) occur once in 150 births and identical twins (single egg divides into two separate embryos, so both have the same sex) once in 400 births (Note! This is not the true values, see Exercise 1.6, page 28, in BDA3). American male singer-actor Elvis Presley (1935 – 1977) had a twin brother who died in birth. Assume that an equal number of boys and girls are born on average. What is the probability that Elvis was an identical twin? Show the steps how you derived the equations to compute that probability. State the problem as a conditional probability and use Bayes’ rule to decompose into simpler terms. \\[ \\begin{aligned} \\Pr(\\text{Elvis was an identical twin}) &amp;= \\\\ \\Pr(\\text{identical} | \\text{twin AND brother}) &amp;= \\frac{\\Pr(\\text{identical})\\Pr(\\text{twin AND bro} | \\text{identical})}{\\Pr(\\text{twin AND bro})} \\\\ \\end{aligned} \\] Each component separately. \\[ \\Pr(\\text{identical}) = \\frac{1}{400} \\\\ \\Pr(\\text{twin AND bro} | \\text{identical}) = \\frac{1}{2} \\\\ \\] \\[ \\begin{aligned} \\Pr(\\text{twin AND bro}) &amp;= \\Pr(\\text{identical AND bro}) + \\Pr(\\text{fraternal AND bro}) \\\\ &amp;= \\frac{1}{400} \\frac{1}{2} + \\frac{1}{150} \\frac{1}{4} \\\\ &amp;= \\frac{7}{2400} \\\\ \\end{aligned} \\] Thus, \\[ \\Pr(\\text{identical} | \\text{twin AND brother}) = \\frac{\\frac{1}{400} \\frac{1}{2}}{\\frac{7}{2400}} = 0.429 \\] Implement this as a function in R that computes the probability. p_identical_twin &lt;- function(fraternal_prob, identical_prob) { (identical_prob * 0.5) / (identical_prob * 0.5 + fraternal_prob * 0.25) } # Tests from provided examples. stopifnot(round(p_identical_twin(1 / 125, 1 / 300), 7) == 0.4545455) stopifnot(round(p_identical_twin(1 / 100, 1 / 500), 7) == 0.2857143) p_identical_twin(fraternal_prob = 1 / 150, identical_prob = 1 / 400) #&gt; [1] 0.4285714 "],["introduction-2.html", "Introduction", " Introduction TODO: add more information here; describe the general structure of the notes. Some exercises from the end of the chapter. "],["chapter-1-exercises.html", "6 Chapter 1 Exercises 6.1 Question 1 6.2 Question 2 6.3 Question 6 6.4 Question 8", " 6 Chapter 1 Exercises 2021-08-19 Complete questions 1-4 and 6-8. 6.1 Question 1 When \\(\\theta = 1\\), then \\(y = N(\\mu = 1, \\sigma)\\) and when \\(\\theta = 2\\), then \\(y = N(\\mu=2, \\sigma)\\). \\(\\Pr(\\theta=1)= \\Pr(\\theta=2) = 0.5\\). a) If \\(\\sigma=2\\) what is the marginal probability density for \\(y\\)? \\[ \\begin{aligned} &amp;= \\Sigma_{\\theta=1}^{\\Theta} \\Pr(\\theta) N(y | \\mu_\\theta, \\sigma) \\\\ &amp;= \\frac{1}{2} N(y|1,2) + \\frac{1}{2} N(y|2,2) \\end{aligned} \\] y &lt;- seq(-6, 10, 0.1) d &lt;- 0.5 * dnorm(y, 1, 2) + 0.5 * dnorm(y, 2, 2) plot( y, d, type = &quot;l&quot;, frame = FALSE, xlab = &quot;y&quot;, ylab = &quot;probability density&quot;, main = &quot;Joint probability density of y&quot; ) b) What is \\(\\Pr(\\theta=1 | y=1)\\) with \\(\\sigma=2\\). Solve using Baye’s rule: \\[ \\begin{aligned} \\Pr(\\theta | y) &amp;= \\frac{\\Pr(\\theta) \\Pr(y | \\theta)}{\\Pr(y)} \\\\ \\Pr(\\theta=1 | y=1) &amp;= \\frac{\\Pr(\\theta=1) \\Pr(y=1 | \\theta=1)}{\\Pr(y=1)} \\\\ \\end{aligned} \\] where \\[ \\Pr(\\theta = 1) = 0.5 \\\\ \\Pr(y=1 | \\theta=1) = N(y=1|1,2) \\\\ \\Pr(y=1) = \\frac{1}{2} N(y=1|1,2) + \\frac{1}{2} N(y=1|2,2) \\] thus \\[ \\begin{aligned} \\Pr(\\theta=1 | y=1) &amp;= \\frac{\\Pr(\\theta=1) \\Pr(y=1 | \\theta=1)}{\\Pr(y=1)} \\\\ &amp;= \\frac{\\frac{1}{2} N(y=1|1,2)}{\\frac{1}{2} N(y=1|1,2) + \\frac{1}{2} N(y=1|2,2)} \\\\ \\end{aligned} \\] (0.5 * dnorm(1, 1, 2)) / (0.5 * dnorm(1, 1, 2) + 0.5 * dnorm(1, 2, 2)) #&gt; [1] 0.5312094 c) Describe the posterior density of \\(\\theta\\) as \\(\\sigma\\) increases or decreases. As \\(\\sigma \\to \\infty\\), the probabilities \\(\\Pr(y|\\theta)\\) and \\(\\Pr(y)\\) become increasingly wide, resulting in the prior probability \\(\\Pr(\\theta)\\) consuming the equation resulting in \\(\\Pr(\\theta=1|y=1) = \\frac{1}{2}\\). This situation would be analogous to having no data. As \\(\\sigma \\to 0\\), the opposite occurs and the prior is overwhelmed by the probability \\(\\Pr(y=1|\\theta=1)\\). Thus \\(\\Pr(\\theta=1|y=1) = 1\\); complete certainty in the value of \\(\\theta\\). This situation would be analogous to collecting a lot of highly homogeneous data. 6.2 Question 2 Conditional means and variances: show that equations 1.8 and 1.9 hold if \\(u\\) is a vector. Equation 1.8: \\(\\text{E}(u) = \\text{E}(\\text{E}(u|v))\\) For a vector \\(u\\), Equation 1.8 would be computed componentwise: \\(\\text{E}(u_i) = \\text{E}(\\text{E}(u_i|v))\\). Equation 1.9: \\(\\text{var}(u) = \\text{E}(\\text{var}(u|v)) + \\text{var}(\\text{E}(u|v))\\) For a vecotr \\(u\\), the diagnoals for Euqation 1.9 would be computed componentwise: \\(\\text{var}(u_i) = \\text{E}(\\text{var}(u_i|v)) + \\text{var}(\\text{E}(u_i|v))\\). For off-diagonals, the result is the covariance between the indeices of \\(u\\): \\(\\text{cov}(u_i, u_j)\\). 6.3 Question 6 Approximately 1/125 of all births are fraternal twins and 1/300 are identical twins. Elvis had a twin brother. What is the probability that Elivs was an identical twin? \\[ \\Pr(\\text{identical twin} | \\text{twin and brother}) = \\frac{\\Pr(\\text{identical twin}) \\Pr(\\text{twin and brother} | \\text{identical twin})}{\\Pr(\\text{twin and brother})} \\\\ \\] \\[ \\begin{aligned} \\Pr(\\text{identical twin}) = \\frac{1}{300} \\\\ \\Pr(\\text{twin and brother} | \\text{identical twin}) = 1 \\\\ \\Pr(\\text{twin and brother}) &amp;= \\Pr(\\text{identical twin}) \\Pr(\\text{boy} | \\text{identical twin}) + \\Pr(\\text{fraternal twin}) \\Pr(\\text{boy} | \\text{fraternal twin}) \\\\ &amp;=\\frac{1}{300} \\times 1 + \\frac{1}{125} \\times \\frac{1}{2} \\end{aligned} \\] \\[ \\begin{aligned} \\Pr(\\text{identical twin} | \\text{twin and brother}) &amp;= \\frac{\\frac{1}{300} \\times 1}{\\frac{1}{300} \\times 1 + \\frac{1}{125} \\times \\frac{1}{2}} \\\\ &amp;= \\frac{\\frac{1}{300}}{\\frac{11}{1500}} \\\\ &amp;= \\frac{5}{11} \\end{aligned} \\] 6.4 Question 8 Subjective probability: discuss the following statement. ‘The probability of event \\(E\\) is considered “subjective” if two rational persons \\(A\\) and \\(B\\) can assign unequal probabilities to \\(E\\), \\(P_A(E)\\) and \\(P_B(E)\\). These probabilities can also be interpreted as “conditional”: \\(P_A(E)\\) = \\(P(E|I_A)\\) and \\(P_B(E) = P(E|I_B)\\), where \\(I_A\\) and \\(I_B\\) represent the knowledge available to persons \\(A\\) and \\(B\\), respectively.’ Apply this idea to the following examples. (a) The probability that a ‘6’ appears when a fair die is rolled, where \\(A\\) observes the outcome of the die roll and \\(B\\) does not. In this case, the statement “the probability of event \\(E\\) is considered”subjective” if two rational persons \\(A\\) and \\(B\\) can assign unequal probabilities to \\(E\\)” does not hold because \\(A\\) knows the value of the die whereas \\(B\\) must guess at random. Thus the probability of the event is not subjective, the two people have different amounts of data. (b) The probability that Brazil wins the next World Cup, where \\(A\\) is ignorant of soccer and \\(B\\) is a knowledgeable sports fan. As the event has yet to occur, the probability of the event is subjective and \\(B\\) has a stronger prior belief than does \\(A\\). "],["the-website.html", "The website", " The website TODO: update this for bookdown format This site is built with ‘distill’ and deployed on GitHub pages. The main point of the website is to display my course notes in a easy to read and search format. It isn’t perfect nor optimal, but there is a good balance of functionality and simplicity. "],["about-me.html", "About me", " About me My name is Joshua Cook and I am (at the time of taking this course and writing this About page) a graduate student at Harvard Medical School. My research is on cancer genetics and I have a specific love of Bayesian modelling. You can peruse more of my projects and other work on my GitHub profile at jhrcook or my website. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
