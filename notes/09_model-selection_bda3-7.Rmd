---
title: "09. Model comparison and selection"
date: "2021-11-09"
output: distill::distill_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, dpi = 300, comment = "#>")
```

## Resources

- BDA3 chapter 7 and [reading instructions](../reading-instructions/BDA3_ch07_reading-instructions.pdf)
- lectures:
  - ['9.1 PSIS-LOO and K-fold-CV'](https://aalto.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=50b2e73f-af0a-4715-b627-ab0200ca7bbd)
  - ['9.2 Model comparison and selection'](https://aalto.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=b0299d53-9454-4e33-9086-ab0200db14ee)
  - ['9.3 Variable selection with projection predictive variable selection'](https://aalto.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=4b6eeb48-ae64-4860-a8c3-ab0200e40ad8)
- slides:
  - [chapter 7b](../slides/slides_ch7b.pdf)
- [Assignment 8](assignments/assignment-08.pdf)

## Notes

> See notes for course section 8 for notes on ch. 7 of *BDA3*.

### Lecture notes

#### 9.1 PSIS-LOO and K-fold CV

##### PSIS-LOO CV

- given some data $x$ and observed $y$
- sample from posterior: $\theta^{(s)} \sim p(\theta | x, y)$
- compute the predictive density for new $\tilde{x}$ and $\tilde{y}$: $p(\tilde{y} | \tilde{x}, x, y) \approx \frac{1}{S} \sum_{s=1}^S p(\tilde{y} | \tilde{x} \theta^{(s)})$
- LOO-CV but weighting the draws using importance sampling:
  - importance ratio: $r_i^{(s)} = p(\theta^{(s)} | x_{-i}, y_{-i}) / p(\theta^{(s)} | x,y)$
    - the ratio of the posterior without $x_i$ and $y_i$ to the full posterior distribution
    - get higher $r_i$ (more importance) for draws $s$ where the probabilities are higher without the data points $i$ than with them
  - $r_i^{(s)} = p(\theta^{(s)} | x_{-i}, y_{-i}) / p(\theta^{(s)} | x,y) \propto 1 / p(y_i|x_i, \theta^{(s)})$
    - because $p(\theta^{(s)} | x,y) = \prod_i^n p(y_i|x_i, \theta^{(s)}) p(\theta^{(s)})$ and the difference for the ratio is that the numerator has one less data point ($-i$) than the denominator
    - therefore, everything cancels out to leave $1 / p(y_i|x_i, \theta^{(s)})$ (multiplied against a constant)
  - easier to work with logarithms: $\log(1/p(y_i|x_i, \theta^{(s)})) = -\log(\text{likelihood}[i])$
- importance-weighted predictive distribution: $p(y_i|x_i, x_{-i}, y_{-i}) \approx \sum_{s=1}^S (w_i^{(s)} p(y_i|x_i, \theta^{(s)}))$
  - where weight $w_i \leftarrow \text{PSIS}(r_i)$
    - Pareto smoothing to help stabilize the importance weights
- can have problem where very large importance weights make it that there are very few effective draws for this calculation
  - most draws will have little effect when there are a few dominating importance ratios
  - can look at the distribution of the weights to see how reliable the importance sampling is
  - compute the effective sample size $n_\text{eff}$ from the importance ratios
- Pareto smoothing
  - can approximate the extreme tail of the importance ratios using a generalized Pareto distribution
  - Pareto dist. has two parameters
    - *scale*
    - *shape* ($\hat{k}$): how many finite moments the weight distribution has
      - if the shape is less than 0.5, then the variance is finite
      - else, the variance may be infinite and CLT may not hold - means the approximation is not reliable
      - in practice, $\hat{k} < 0.7$ is okay
- plot the Pareto $\hat{k}$ and $n_\text{eff}$ for each data point (will usually be inversely correlated)

##### K-fold CV

- varieties
  - can approximate LOO by removing some $n$ number of data points
  - leave-one-*group*-out for hierarchical models
  - leave-one-*block*-out for time series
    - remove all data from a certain time point or range

#### 9.2 Model comparison and selection



#### 9.3 Variable selection with projection predictive variable selection
